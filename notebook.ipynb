{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi Author Writing Style Analysis\n",
    "\n",
    "The following notebook presents three different approaches to the problem of distingushing when in a sequence of paragraphs,\n",
    "the author changes. The first approach disregrads the order of the paragraphs, opting instead to view samples as pairs of paragraphs.\n",
    "It processes the paragraphs with a siamese network, which is a neural network that takes two inputs and outputs a single value.\n",
    "The second approach adds a recurrent layer to the siamese network, allowing it to take into account a sequence of paragraphs.\n",
    "The third approach builds on the second by augmenting the input with a manually engineered feature vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import pickle\n",
    "import torch\n",
    "import warnings\n",
    "from functools import partial\n",
    "from torch.nn import functional as F\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "import itertools\n",
    "from tqdm import tqdm\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.metrics import f1_score, accuracy_score\n",
    "import pandas as pd\n",
    "import nltk\n",
    "import yaml\n",
    "from src.utils import get_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "warnings.filterwarnings('ignore')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data\n",
    "\n",
    "We load the data (uncomment to reconstruct the data from the raw files),\n",
    "and make two data batch loaders:\n",
    "\n",
    "1. pairs of paragraphs, which will be used for our baseline siamese network.\n",
    "2. Sequences of paragraphs, which will be used for our recurrent siamese network.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nltk.download('averaged_perceptron_tagger')\n",
    "# data = { str(i): get_data(i) for i in range(1, 4) }\n",
    "# pickle.dump(data, open('data/data.pkl', 'wb'))\n",
    "data = pickle.load(open('data/data.pkl', 'rb'))\n",
    "dataset_1, dataset_2, dataset_3 = data['1'], data['2'], data['3']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def paired_samples(data_split, modality):\n",
    "    \"\"\"turns data set into pair of consectuve sentences (flattens multi paragraph samples into pairs)\"\"\"\n",
    "    pairs = []\n",
    "    for problem_id in data_split.keys():\n",
    "        semantic = data_split[problem_id]['semantic']\n",
    "        syntactic = data_split[problem_id]['syntactic']\n",
    "        # concatenate all symantic and syntactic features into one vecntor per sample\n",
    "        if modality == 'both':\n",
    "            texts = [np.concatenate([semantic[i], syntactic[i]]) for i in range(len(semantic))]\n",
    "        elif modality == 'semantic':\n",
    "            texts = data_split[problem_id]['semantic']\n",
    "        elif modality == 'syntactic':\n",
    "            texts = data_split[problem_id]['syntactic']\n",
    "        else:\n",
    "            raise ValueError(f'invalid modality {modality}')\n",
    "        targets = data_split[problem_id]['truth']['changes']\n",
    "        if len(texts) - 1 != len(targets):\n",
    "            # TODO: fix. a few of the samples have more than one paragraph, making .readlines() wrong\n",
    "            # print(f'problem {problem_id} has {len(texts)} texts and {len(targets)} targets')\n",
    "            continue\n",
    "        for target, text1, text2 in zip(targets, texts[:-1], texts[1:]):\n",
    "            pairs.append((text1, text2, target))\n",
    "    random.shuffle(pairs)\n",
    "    return pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pair_batches(data_split, modality, batch_size=32):\n",
    "    pairs = paired_samples(data_split, modality)\n",
    "    while True:\n",
    "        # perm = np.random.permutation(len(pairs))\n",
    "        x1 = torch.tensor(np.array([p[0] for p in pairs])).float().to(device)\n",
    "        x2 = torch.tensor(np.array([p[1] for p in pairs])).float().to(device)\n",
    "        y = torch.tensor(np.array([p[2] for p in pairs])).float().to(device)\n",
    "        perm = torch.randperm(len(pairs))\n",
    "        for i in range(0, len(pairs), batch_size):\n",
    "            batch = perm[i:i+batch_size]\n",
    "            yield (x1[batch], x2[batch]), y[batch]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sequence_batches(data_split, modality, batch_size=32):\n",
    "    \"\"\"turns data set into sequence of sentences (flattens multi paragraph samples into sequence)\"\"\"\n",
    "    x, y = [], []\n",
    "    for problem_id in data_split.keys():\n",
    "        semantic = data_split[problem_id]['semantic']\n",
    "        syntactic = data_split[problem_id]['syntactic']\n",
    "        # concatenate all symantic and syntactic features into one vecntor per sample\n",
    "        if modality == 'both':\n",
    "            texts = [np.concatenate([semantic[i], syntactic[i]]) for i in range(len(semantic))]\n",
    "        elif modality == 'syntactic':\n",
    "            texts = data_split[problem_id]['syntactic']\n",
    "        elif modality == 'semantic':\n",
    "            texts = data_split[problem_id]['semantic']\n",
    "        else:\n",
    "            raise ValueError(f'invalid modality {modality}')\n",
    "        targets = data_split[problem_id]['truth']['changes']\n",
    "        if len(texts) - 1 != len(targets):\n",
    "            continue\n",
    "        x.append(torch.tensor(texts))\n",
    "        y.append(torch.tensor(targets))\n",
    "    while True:\n",
    "        perm = torch.randperm(len(x))\n",
    "        for i in range(0, len(x), batch_size):\n",
    "            batch = perm[i:i+batch_size]\n",
    "            x_batch = [x[i] for i in batch]\n",
    "            y_batch = [y[i] for i in batch]\n",
    "            y_batch = torch.cat(y_batch, dim=0).to(device)\n",
    "            # pad with zero vectors\n",
    "            x_batch = pad_sequence(x_batch, batch_first=True, padding_value=0).to(device)\n",
    "            x_batch = x_batch.float()\n",
    "            yield x_batch, y_batch"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Models\n",
    "\n",
    "We make our two models, the siamese network and the recurrent siamese network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def soft_f1_loss(y_pred, y_true):\n",
    "    \"\"\"computes soft f1 loss\"\"\"\n",
    "    tp = (y_true * y_pred).sum(dim=1)\n",
    "    tn = ((1 - y_true) * (1 - y_pred)).sum(dim=1)\n",
    "    fp = ((1 - y_true) * y_pred).sum(dim=1)\n",
    "    fn = (y_true * (1 - y_pred)).sum(dim=1)\n",
    "    epsilon = 1e-7\n",
    "    precision = tp / (tp + fp + epsilon)\n",
    "    recall = tp / (tp + fn + epsilon)\n",
    "    f1 = 2 * precision * recall / (precision + recall + epsilon)\n",
    "    return 1 - f1.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SiameseNet(torch.nn.Module):\n",
    "    def __init__(self, config, embed_dim):\n",
    "        super().__init__()\n",
    "        self.embed_dim = embed_dim\n",
    "        self.hidden_dim = config['hidden_dim']\n",
    "        self.linear1 = torch.nn.Linear(self.embed_dim, self.hidden_dim)\n",
    "        self.linear2 = torch.nn.Linear(self.hidden_dim, self.hidden_dim)\n",
    "        self.linear3 = torch.nn.Linear(self.hidden_dim, 1)\n",
    "        self.dropout = torch.nn.Dropout(config['dropout'])\n",
    "        self.sigmoid = torch.nn.Sigmoid()\n",
    "    \n",
    "    def forward(self, x, y=None):\n",
    "        x1, x2 = x\n",
    "        x1_hat = self.dropout(x1)\n",
    "        x1_hat = self.linear1(x1_hat)\n",
    "        x1_hat = F.gelu(x1_hat)\n",
    "        x2_hat = self.dropout(x2)\n",
    "        x2_hat = self.linear1(x2_hat)\n",
    "        x2_hat = F.gelu(x2_hat)\n",
    "        y_hat = torch.abs(x1_hat - x2_hat)\n",
    "        y_hat = self.linear2(y_hat)\n",
    "        y_hat = F.gelu(y_hat)\n",
    "        y_hat = self.dropout(y_hat)\n",
    "        y_hat = self.linear3(y_hat)\n",
    "        y_hat = self.sigmoid(y_hat)\n",
    "        if y is not None:\n",
    "            # loss = torch.nn.functional.binary_cross_entropy(y_hat, y.float().unsqueeze(1))\n",
    "            # switch to soft f1 loss\n",
    "            # bce loss\n",
    "            loss = torch.nn.functional.binary_cross_entropy(y_hat, y.float().unsqueeze(1))\n",
    "            return y_hat, loss\n",
    "        return y_hat\n",
    "\n",
    "    def predict(self, x):\n",
    "        y_hat = self.forward(x)\n",
    "        y_hat = y_hat.squeeze(1)\n",
    "        y_hat = (y_hat > 0.5).int()\n",
    "        return y_hat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RecurrentNet(torch.nn.Module):\n",
    "    def __init__(self, config, embed_dim):\n",
    "        super().__init__()\n",
    "        self.embed_dim = embed_dim\n",
    "        self.hidden_dim = config['hidden_dim']\n",
    "        self.linear1 = torch.nn.Linear(self.embed_dim, self.hidden_dim)\n",
    "        self.gru = torch.nn.GRU(self.hidden_dim, self.hidden_dim, batch_first=True)\n",
    "        self.linear2 = torch.nn.Linear(self.hidden_dim, 1)\n",
    "        self.dropout = torch.nn.Dropout(config['dropout'])\n",
    "    \n",
    "    def forward(self, x, y=None):\n",
    "        # x is tensor of shape (batch_size, seq_len, embed_dim)\n",
    "        mask = self.x_mask(x)\n",
    "        y_hat = self.dropout(x)\n",
    "        y_hat = y_hat.reshape(-1, self.embed_dim)\n",
    "        y_hat = self.linear1(y_hat) \n",
    "        y_hat = F.gelu(y_hat)\n",
    "        y_hat = y_hat.reshape(-1, x.shape[1], self.hidden_dim)\n",
    "        # y_hat = y_hat[:, 1:, :] - y_hat[:, :-1, :]  # take difference between consecutive sentences (Siamese net)\n",
    "        y_hat = self.gru(y_hat)[0]\n",
    "        y_hat = y_hat.reshape(-1, self.hidden_dim)  # flatten for masking and y_hat\n",
    "        y_hat = y_hat[mask]\n",
    "        y_hat = F.gelu(y_hat)\n",
    "        y_hat = self.linear2(y_hat)\n",
    "        y_hat = F.sigmoid(y_hat)\n",
    "        y_hat = y_hat.squeeze(1)\n",
    "        if y is not None:\n",
    "            try: \n",
    "                # soft f1 loss\n",
    "                # loss = soft_f1_loss(y_hat, y.float())\n",
    "                # bce loss\n",
    "                loss = torch.nn.functional.binary_cross_entropy(y_hat, y.float())\n",
    "            except ValueError:\n",
    "                return y_hat, None \n",
    "            return y_hat, loss\n",
    "        return y_hat\n",
    "\n",
    "    def x_mask(self, x):\n",
    "        \"\"\"returns mask of shape (batch_size, seq_len)\"\"\"\n",
    "        mask = torch.sum(x, dim=2) != 0\n",
    "        mask[:, 0] = False\n",
    "        mask = mask.view(-1)\n",
    "        return mask\n",
    "\n",
    "    def predict(self, x):\n",
    "        y_hat = self.forward(x)\n",
    "        y_hat = (y_hat > 0.5).int()\n",
    "        return y_hat"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training\n",
    "\n",
    "We define our training and evaluation functions, for use by both models, and all three datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_curve(metrics):\n",
    "    plt.style.use('dark_background')\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(16, 4))\n",
    "    axes[0].plot(metrics['train_loss'], label='train')\n",
    "    axes[0].plot(metrics['valid_loss'], label='val')\n",
    "    axes[0].set_title('Loss')\n",
    "    axes[0].legend()\n",
    "    axes[1].plot(metrics['train_f1'], label='train')\n",
    "    axes[1].plot(metrics['valid_f1'], label='val')\n",
    "    axes[1].set_title('F1')\n",
    "    axes[1].legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(metrics, model, train_batches, valid_batches, steps=10):\n",
    "    for batch_name, batches in [('train', train_batches), ('valid', valid_batches)]:\n",
    "        loss, f1, acc = evaluate_split(model, batches, steps=steps)\n",
    "        metrics[batch_name + '_loss'].append(loss)\n",
    "        metrics[batch_name + '_f1'].append(f1)\n",
    "        metrics[batch_name + '_acc'].append(acc)\n",
    "        model.eval()\n",
    "    model.train()\n",
    "    return metrics\n",
    "\n",
    "def evaluate_split(model, batches, steps):\n",
    "    f1_scores, losses, acc_scores = [], [], []\n",
    "    for i in range(steps):\n",
    "        x, y = next(batches)\n",
    "        y_hat, loss = model(x, y)\n",
    "        if loss is None:\n",
    "            continue\n",
    "        losses.append(loss.item())\n",
    "        y_hat = model.predict(x).cpu().numpy().astype(int)\n",
    "        y = y.cpu().numpy().astype(int)\n",
    "        f1_scores.append(f1_score(y, y_hat))\n",
    "        acc_scores.append(accuracy_score(y, y_hat))\n",
    "    return np.mean(losses), np.mean(f1_scores), np.mean(acc_scores)\n",
    "\n",
    "def train(model, optimizer, train_batches, valid_batches=None, batch_size=32, n_steps=1000):\n",
    "    # returns metrics and final scores, if doing validation, else returns final model for testing\n",
    "    metrics = {'train_loss': [], 'train_f1': [], 'train_acc': [], 'valid_loss': [], 'valid_f1': [], 'valid_acc': []}\n",
    "    for i in range(n_steps):\n",
    "        x, y = next(train_batches)\n",
    "        y_hat, loss = model(x, y)\n",
    "        if loss is None:  # there is an extremly rare bug where y_hat is one short of y FIXME.\n",
    "            continue\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if valid_batches and i % (n_steps // 100) == 0:\n",
    "            metrics = evaluate(metrics, model, train_batches, valid_batches)\n",
    "    if valid_batches:\n",
    "        final = evaluate(metrics, model, train_batches, valid_batches, steps=4200 // batch_size)\n",
    "        return metrics, {k: v[-1] for k, v in final.items()}\n",
    "    return model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment\n",
    "\n",
    "To do hyper paramter tuning, and test performance of our models on our three datasets, we define the experiment functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hyper_params():\n",
    "    \"\"\"return random hyperparameters\"\"\"\n",
    "    return {\n",
    "        'lr': 10 ** random.choice([-3, -4, -5]),\n",
    "        'dropout': random.choice([0.1, 0.2, 0.3]),\n",
    "        'hidden_dim': random.choice([32, 64, 128, 256]),\n",
    "        'batch_size': random.choice([16, 32, 64]),\n",
    "        'n_steps': random.choice([2000, 4000, 6000, 8000]),\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hyper_param_search(model_fn, batch_fn, dataset, modality, n_trials=10):\n",
    "    \"\"\"search hyperparameters for a given model and dataset\"\"\"\n",
    "    if modality == 'both':\n",
    "        embed_dim = 384 + 61\n",
    "    elif modality == 'semantic':\n",
    "        embed_dim = 384\n",
    "    elif modality == 'syntactic':\n",
    "        embed_dim = 61\n",
    "    else:\n",
    "        raise ValueError(f'invalid modality {modality}')\n",
    "    hyper_param_metrics = []\n",
    "    training_metrics_list = []\n",
    "    for i in tqdm(range(n_trials)):\n",
    "        config = hyper_params()\n",
    "        model = model_fn(config, embed_dim=embed_dim).to(device)\n",
    "        optimizer = torch.optim.AdamW(model.parameters(), lr=config['lr'])\n",
    "        train_batches = batch_fn(dataset['train'], batch_size=config['batch_size'])\n",
    "        valid_batches = batch_fn(dataset['valid'], batch_size=config['batch_size'])\n",
    "        training_metrics, final= train(model, optimizer, train_batches, valid_batches, config['batch_size'], config['n_steps'])\n",
    "        training_metrics_list.append(training_metrics)\n",
    "        hyper_param_metrics.append({**config, **final})\n",
    "    df = pd.DataFrame(hyper_param_metrics).sort_values('valid_f1', ascending=False)\n",
    "    return df, training_metrics_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_combinations():\n",
    "    datasets = [(dataset_2, 'dataset_2'), (dataset_3, 'dataset_3'), (dataset_1, 'dataset_1')]\n",
    "    models = [(RecurrentNet, get_sequence_batches), (SiameseNet, get_pair_batches)]\n",
    "    syntac_bools = ['both', 'semantic', 'syntactic']\n",
    "    return list(itertools.product(datasets, models, syntac_bools))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def experiment(n_trials=10):\n",
    "    for (ds, ds_name), (model, batch_fn), modality in get_combinations():\n",
    "        print(f'{model.__name__} on {ds_name} with {modality}')\n",
    "        batch_fn = partial(batch_fn, modality=modality)\n",
    "        df, training_metrics_list = hyper_param_search(model, batch_fn, ds, modality, n_trials=n_trials)\n",
    "        df.round(4).to_csv(f'results/true_siam_{model.__name__}_{ds_name}_{modality}.csv')\n",
    "        pickle.dump(training_metrics_list, open(f'results/true_siam_training_{model.__name__}_{ds_name}_{modality}.pkl', 'wb'))\n",
    "        print(f'best f1 score: {df.iloc[0][\"valid_f1\"]}')\n",
    "        print(f'best accuracy score: {df.iloc[0][\"valid_acc\"]}')\n",
    "        print()        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RecurrentNet on dataset_2 with both\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:59<00:00, 59.82s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best f1 score: 0.5463287977288087\n",
      "best accuracy score: 0.6253650212219702\n",
      "\n",
      "RecurrentNet on dataset_2 with semantic\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1 [00:04<?, ?it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[37], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m experiment(\u001b[39m1\u001b[39;49m)\n",
      "Cell \u001b[0;32mIn[35], line 5\u001b[0m, in \u001b[0;36mexperiment\u001b[0;34m(n_trials)\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39m{\u001b[39;00mmodel\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m on \u001b[39m\u001b[39m{\u001b[39;00mds_name\u001b[39m}\u001b[39;00m\u001b[39m with \u001b[39m\u001b[39m{\u001b[39;00mmodality\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m)\n\u001b[1;32m      4\u001b[0m batch_fn \u001b[39m=\u001b[39m partial(batch_fn, modality\u001b[39m=\u001b[39mmodality)\n\u001b[0;32m----> 5\u001b[0m df, training_metrics_list \u001b[39m=\u001b[39m hyper_param_search(model, batch_fn, ds, modality, n_trials\u001b[39m=\u001b[39;49mn_trials)\n\u001b[1;32m      6\u001b[0m df\u001b[39m.\u001b[39mround(\u001b[39m4\u001b[39m)\u001b[39m.\u001b[39mto_csv(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mresults/true_siam_\u001b[39m\u001b[39m{\u001b[39;00mmodel\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m_\u001b[39m\u001b[39m{\u001b[39;00mds_name\u001b[39m}\u001b[39;00m\u001b[39m_\u001b[39m\u001b[39m{\u001b[39;00mmodality\u001b[39m}\u001b[39;00m\u001b[39m.csv\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m      7\u001b[0m pickle\u001b[39m.\u001b[39mdump(training_metrics_list, \u001b[39mopen\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mresults/true_siam_training_\u001b[39m\u001b[39m{\u001b[39;00mmodel\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m_\u001b[39m\u001b[39m{\u001b[39;00mds_name\u001b[39m}\u001b[39;00m\u001b[39m_\u001b[39m\u001b[39m{\u001b[39;00mmodality\u001b[39m}\u001b[39;00m\u001b[39m.pkl\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mwb\u001b[39m\u001b[39m'\u001b[39m))\n",
      "Cell \u001b[0;32mIn[31], line 19\u001b[0m, in \u001b[0;36mhyper_param_search\u001b[0;34m(model_fn, batch_fn, dataset, modality, n_trials)\u001b[0m\n\u001b[1;32m     17\u001b[0m train_batches \u001b[39m=\u001b[39m batch_fn(dataset[\u001b[39m'\u001b[39m\u001b[39mtrain\u001b[39m\u001b[39m'\u001b[39m], batch_size\u001b[39m=\u001b[39mconfig[\u001b[39m'\u001b[39m\u001b[39mbatch_size\u001b[39m\u001b[39m'\u001b[39m])\n\u001b[1;32m     18\u001b[0m valid_batches \u001b[39m=\u001b[39m batch_fn(dataset[\u001b[39m'\u001b[39m\u001b[39mvalid\u001b[39m\u001b[39m'\u001b[39m], batch_size\u001b[39m=\u001b[39mconfig[\u001b[39m'\u001b[39m\u001b[39mbatch_size\u001b[39m\u001b[39m'\u001b[39m])\n\u001b[0;32m---> 19\u001b[0m training_metrics, final\u001b[39m=\u001b[39m train(model, optimizer, train_batches, valid_batches, config[\u001b[39m'\u001b[39;49m\u001b[39mbatch_size\u001b[39;49m\u001b[39m'\u001b[39;49m], config[\u001b[39m'\u001b[39;49m\u001b[39mn_steps\u001b[39;49m\u001b[39m'\u001b[39;49m])\n\u001b[1;32m     20\u001b[0m training_metrics_list\u001b[39m.\u001b[39mappend(training_metrics)\n\u001b[1;32m     21\u001b[0m hyper_param_metrics\u001b[39m.\u001b[39mappend({\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mconfig, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mfinal})\n",
      "Cell \u001b[0;32mIn[29], line 37\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(model, optimizer, train_batches, valid_batches, batch_size, n_steps)\u001b[0m\n\u001b[1;32m     35\u001b[0m     optimizer\u001b[39m.\u001b[39mstep()\n\u001b[1;32m     36\u001b[0m     \u001b[39mif\u001b[39;00m valid_batches \u001b[39mand\u001b[39;00m i \u001b[39m%\u001b[39m (n_steps \u001b[39m/\u001b[39m\u001b[39m/\u001b[39m \u001b[39m100\u001b[39m) \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[0;32m---> 37\u001b[0m         metrics \u001b[39m=\u001b[39m evaluate(metrics, model, train_batches, valid_batches)\n\u001b[1;32m     38\u001b[0m \u001b[39mif\u001b[39;00m valid_batches:\n\u001b[1;32m     39\u001b[0m     final \u001b[39m=\u001b[39m evaluate(metrics, model, train_batches, valid_batches, steps\u001b[39m=\u001b[39m\u001b[39m4200\u001b[39m \u001b[39m/\u001b[39m\u001b[39m/\u001b[39m batch_size)\n",
      "Cell \u001b[0;32mIn[29], line 3\u001b[0m, in \u001b[0;36mevaluate\u001b[0;34m(metrics, model, train_batches, valid_batches, steps)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mevaluate\u001b[39m(metrics, model, train_batches, valid_batches, steps\u001b[39m=\u001b[39m\u001b[39m10\u001b[39m):\n\u001b[1;32m      2\u001b[0m     \u001b[39mfor\u001b[39;00m batch_name, batches \u001b[39min\u001b[39;00m [(\u001b[39m'\u001b[39m\u001b[39mtrain\u001b[39m\u001b[39m'\u001b[39m, train_batches), (\u001b[39m'\u001b[39m\u001b[39mvalid\u001b[39m\u001b[39m'\u001b[39m, valid_batches)]:\n\u001b[0;32m----> 3\u001b[0m         loss, f1, acc \u001b[39m=\u001b[39m evaluate_split(model, batches, steps\u001b[39m=\u001b[39;49msteps)\n\u001b[1;32m      4\u001b[0m         metrics[batch_name \u001b[39m+\u001b[39m \u001b[39m'\u001b[39m\u001b[39m_loss\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39mappend(loss)\n\u001b[1;32m      5\u001b[0m         metrics[batch_name \u001b[39m+\u001b[39m \u001b[39m'\u001b[39m\u001b[39m_f1\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39mappend(f1)\n",
      "Cell \u001b[0;32mIn[29], line 19\u001b[0m, in \u001b[0;36mevaluate_split\u001b[0;34m(model, batches, steps)\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[39mcontinue\u001b[39;00m\n\u001b[1;32m     18\u001b[0m losses\u001b[39m.\u001b[39mappend(loss\u001b[39m.\u001b[39mitem())\n\u001b[0;32m---> 19\u001b[0m y_hat \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39;49mpredict(x)\u001b[39m.\u001b[39mcpu()\u001b[39m.\u001b[39mnumpy()\u001b[39m.\u001b[39mastype(\u001b[39mint\u001b[39m)\n\u001b[1;32m     20\u001b[0m y \u001b[39m=\u001b[39m y\u001b[39m.\u001b[39mcpu()\u001b[39m.\u001b[39mnumpy()\u001b[39m.\u001b[39mastype(\u001b[39mint\u001b[39m)\n\u001b[1;32m     21\u001b[0m f1_scores\u001b[39m.\u001b[39mappend(f1_score(y, y_hat))\n",
      "Cell \u001b[0;32mIn[27], line 47\u001b[0m, in \u001b[0;36mRecurrentNet.predict\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mpredict\u001b[39m(\u001b[39mself\u001b[39m, x):\n\u001b[0;32m---> 47\u001b[0m     y_hat \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mforward(x)\n\u001b[1;32m     48\u001b[0m     y_hat \u001b[39m=\u001b[39m (y_hat \u001b[39m>\u001b[39m \u001b[39m0.5\u001b[39m)\u001b[39m.\u001b[39mint()\n\u001b[1;32m     49\u001b[0m     \u001b[39mreturn\u001b[39;00m y_hat\n",
      "Cell \u001b[0;32mIn[27], line 20\u001b[0m, in \u001b[0;36mRecurrentNet.forward\u001b[0;34m(self, x, y)\u001b[0m\n\u001b[1;32m     18\u001b[0m y_hat \u001b[39m=\u001b[39m y_hat\u001b[39m.\u001b[39mreshape(\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m, x\u001b[39m.\u001b[39mshape[\u001b[39m1\u001b[39m], \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhidden_dim)\n\u001b[1;32m     19\u001b[0m y_hat \u001b[39m=\u001b[39m y_hat[:, \u001b[39m1\u001b[39m:, :] \u001b[39m-\u001b[39m y_hat[:, :\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m, :]  \u001b[39m# take difference between consecutive sentences (Siamese net)\u001b[39;00m\n\u001b[0;32m---> 20\u001b[0m y_hat \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgru(y_hat)[\u001b[39m0\u001b[39m]\n\u001b[1;32m     21\u001b[0m y_hat \u001b[39m=\u001b[39m y_hat\u001b[39m.\u001b[39mreshape(\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhidden_dim)  \u001b[39m# flatten for masking and y_hat\u001b[39;00m\n\u001b[1;32m     22\u001b[0m y_hat \u001b[39m=\u001b[39m y_hat[mask]\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/torch/nn/modules/rnn.py:950\u001b[0m, in \u001b[0;36mGRU.forward\u001b[0;34m(self, input, hx)\u001b[0m\n\u001b[1;32m    948\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcheck_forward_args(\u001b[39minput\u001b[39m, hx, batch_sizes)\n\u001b[1;32m    949\u001b[0m \u001b[39mif\u001b[39;00m batch_sizes \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 950\u001b[0m     result \u001b[39m=\u001b[39m _VF\u001b[39m.\u001b[39;49mgru(\u001b[39minput\u001b[39;49m, hx, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_flat_weights, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mnum_layers,\n\u001b[1;32m    951\u001b[0m                      \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdropout, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtraining, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbidirectional, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbatch_first)\n\u001b[1;32m    952\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    953\u001b[0m     result \u001b[39m=\u001b[39m _VF\u001b[39m.\u001b[39mgru(\u001b[39minput\u001b[39m, batch_sizes, hx, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_flat_weights, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbias,\n\u001b[1;32m    954\u001b[0m                      \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnum_layers, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdropout, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtraining, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbidirectional)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "experiment(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def top_rows_for_each_dataset():\n",
    "    out = {'dataset_1': [], 'dataset_2': [], 'dataset_3': []}\n",
    "    for (ds, ds_name), (model, batch_fn), modality in get_combinations():\n",
    "        df = pd.read_csv(f'results/true_siam_{model.__name__}_{ds_name}_{modality}.csv')\n",
    "        row = df.iloc[0]\n",
    "        meta = {'model': model.__name__, 'modality': modality}\n",
    "        out[ds_name].append({**meta, **row})\n",
    "    for ds_name in out.keys():\n",
    "        df = pd.DataFrame(out[ds_name])\n",
    "        df = df.sort_values('valid_f1', ascending=False)\n",
    "        df.round(4).to_csv(f'results/true_siam_top_{ds_name}_hyperparams.csv', index=False)\n",
    "    out = {k: pd.DataFrame(v) for k, v in out.items()}\n",
    "    return out\n",
    "hyperparam_search_result = top_rows_for_each_dataset()\n",
    "\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_config_file(best_row_for_each_mode_and_dataset):\n",
    "    \"\"\"generate a config file with the best hyper paramters for each model and dataset (have it all in one file)\"\"\"\n",
    "    config = {}\n",
    "    for ds_name in best_row_for_each_mode_and_dataset.keys():\n",
    "        df = best_row_for_each_mode_and_dataset[ds_name]\n",
    "        for i, row in df.iterrows():\n",
    "            hyper_params = {'lr': row['lr'],\n",
    "                            'dropout': row['dropout'],\n",
    "                            'hidden_dim': int(row['hidden_dim']),\n",
    "                            'batch_size': int(row['batch_size']),\n",
    "                            'n_steps': int(row['n_steps'])}\n",
    "            model = row['model']\n",
    "            modality = row['modality']\n",
    "            config[f'{ds_name}_{model}_{modality}'] = hyper_params\n",
    "    with open('config.yaml', 'w') as f:\n",
    "        yaml.dump(config, f)\n",
    "\n",
    "generate_config_file(hyperparam_search_result)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 18/18 [05:23<00:00, 17.97s/it]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>dataset</th>\n",
       "      <th>modality</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>train_f1</th>\n",
       "      <th>train_acc</th>\n",
       "      <th>test_loss</th>\n",
       "      <th>test_f1</th>\n",
       "      <th>test_acc</th>\n",
       "      <th>n_params</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>RecurrentSiameseNet</td>\n",
       "      <td>dataset_2</td>\n",
       "      <td>both</td>\n",
       "      <td>0.4996</td>\n",
       "      <td>0.7110</td>\n",
       "      <td>0.7344</td>\n",
       "      <td>0.5539</td>\n",
       "      <td>0.6496</td>\n",
       "      <td>0.6983</td>\n",
       "      <td>20641</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>RecurrentSiameseNet</td>\n",
       "      <td>dataset_2</td>\n",
       "      <td>semantic</td>\n",
       "      <td>0.5842</td>\n",
       "      <td>0.6588</td>\n",
       "      <td>0.6714</td>\n",
       "      <td>0.6066</td>\n",
       "      <td>0.6150</td>\n",
       "      <td>0.6501</td>\n",
       "      <td>18689</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>RecurrentSiameseNet</td>\n",
       "      <td>dataset_2</td>\n",
       "      <td>syntactic</td>\n",
       "      <td>0.5615</td>\n",
       "      <td>0.6761</td>\n",
       "      <td>0.6911</td>\n",
       "      <td>0.5757</td>\n",
       "      <td>0.6486</td>\n",
       "      <td>0.6796</td>\n",
       "      <td>107137</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>SiameseNet</td>\n",
       "      <td>dataset_2</td>\n",
       "      <td>both</td>\n",
       "      <td>0.5632</td>\n",
       "      <td>0.6789</td>\n",
       "      <td>0.6627</td>\n",
       "      <td>0.5719</td>\n",
       "      <td>0.6677</td>\n",
       "      <td>0.6652</td>\n",
       "      <td>73729</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>SiameseNet</td>\n",
       "      <td>dataset_2</td>\n",
       "      <td>semantic</td>\n",
       "      <td>0.4926</td>\n",
       "      <td>0.6880</td>\n",
       "      <td>0.7502</td>\n",
       "      <td>0.6327</td>\n",
       "      <td>0.5726</td>\n",
       "      <td>0.6585</td>\n",
       "      <td>28865</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>SiameseNet</td>\n",
       "      <td>dataset_2</td>\n",
       "      <td>syntactic</td>\n",
       "      <td>0.5765</td>\n",
       "      <td>0.6589</td>\n",
       "      <td>0.6668</td>\n",
       "      <td>0.5829</td>\n",
       "      <td>0.6060</td>\n",
       "      <td>0.6406</td>\n",
       "      <td>24577</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>RecurrentSiameseNet</td>\n",
       "      <td>dataset_3</td>\n",
       "      <td>both</td>\n",
       "      <td>0.6453</td>\n",
       "      <td>0.5927</td>\n",
       "      <td>0.6248</td>\n",
       "      <td>0.6632</td>\n",
       "      <td>0.5774</td>\n",
       "      <td>0.6154</td>\n",
       "      <td>156289</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>RecurrentSiameseNet</td>\n",
       "      <td>dataset_3</td>\n",
       "      <td>semantic</td>\n",
       "      <td>0.6547</td>\n",
       "      <td>0.5506</td>\n",
       "      <td>0.6067</td>\n",
       "      <td>0.6731</td>\n",
       "      <td>0.5254</td>\n",
       "      <td>0.5801</td>\n",
       "      <td>18689</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>RecurrentSiameseNet</td>\n",
       "      <td>dataset_3</td>\n",
       "      <td>syntactic</td>\n",
       "      <td>0.6689</td>\n",
       "      <td>0.5301</td>\n",
       "      <td>0.5927</td>\n",
       "      <td>0.6752</td>\n",
       "      <td>0.5051</td>\n",
       "      <td>0.5772</td>\n",
       "      <td>410881</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>SiameseNet</td>\n",
       "      <td>dataset_3</td>\n",
       "      <td>both</td>\n",
       "      <td>0.6642</td>\n",
       "      <td>0.4908</td>\n",
       "      <td>0.5992</td>\n",
       "      <td>0.6668</td>\n",
       "      <td>0.4534</td>\n",
       "      <td>0.5826</td>\n",
       "      <td>32769</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>SiameseNet</td>\n",
       "      <td>dataset_3</td>\n",
       "      <td>semantic</td>\n",
       "      <td>0.5596</td>\n",
       "      <td>0.7238</td>\n",
       "      <td>0.7170</td>\n",
       "      <td>0.7615</td>\n",
       "      <td>0.6039</td>\n",
       "      <td>0.5826</td>\n",
       "      <td>65921</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>SiameseNet</td>\n",
       "      <td>dataset_3</td>\n",
       "      <td>syntactic</td>\n",
       "      <td>0.6873</td>\n",
       "      <td>0.4503</td>\n",
       "      <td>0.5425</td>\n",
       "      <td>0.6900</td>\n",
       "      <td>0.4151</td>\n",
       "      <td>0.5223</td>\n",
       "      <td>81921</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>RecurrentSiameseNet</td>\n",
       "      <td>dataset_1</td>\n",
       "      <td>both</td>\n",
       "      <td>0.1305</td>\n",
       "      <td>0.9714</td>\n",
       "      <td>0.9488</td>\n",
       "      <td>0.2395</td>\n",
       "      <td>0.9475</td>\n",
       "      <td>0.9070</td>\n",
       "      <td>509185</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>RecurrentSiameseNet</td>\n",
       "      <td>dataset_1</td>\n",
       "      <td>semantic</td>\n",
       "      <td>0.2850</td>\n",
       "      <td>0.9373</td>\n",
       "      <td>0.8851</td>\n",
       "      <td>0.3680</td>\n",
       "      <td>0.9236</td>\n",
       "      <td>0.8600</td>\n",
       "      <td>148481</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>RecurrentSiameseNet</td>\n",
       "      <td>dataset_1</td>\n",
       "      <td>syntactic</td>\n",
       "      <td>0.2095</td>\n",
       "      <td>0.9529</td>\n",
       "      <td>0.9163</td>\n",
       "      <td>0.2535</td>\n",
       "      <td>0.9492</td>\n",
       "      <td>0.9112</td>\n",
       "      <td>8353</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>SiameseNet</td>\n",
       "      <td>dataset_1</td>\n",
       "      <td>both</td>\n",
       "      <td>0.1839</td>\n",
       "      <td>0.9571</td>\n",
       "      <td>0.9234</td>\n",
       "      <td>0.2548</td>\n",
       "      <td>0.9422</td>\n",
       "      <td>0.8973</td>\n",
       "      <td>180225</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>SiameseNet</td>\n",
       "      <td>dataset_1</td>\n",
       "      <td>semantic</td>\n",
       "      <td>0.0761</td>\n",
       "      <td>0.9821</td>\n",
       "      <td>0.9692</td>\n",
       "      <td>0.2174</td>\n",
       "      <td>0.9496</td>\n",
       "      <td>0.9129</td>\n",
       "      <td>28865</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>SiameseNet</td>\n",
       "      <td>dataset_1</td>\n",
       "      <td>syntactic</td>\n",
       "      <td>0.2330</td>\n",
       "      <td>0.9477</td>\n",
       "      <td>0.9068</td>\n",
       "      <td>0.2764</td>\n",
       "      <td>0.9375</td>\n",
       "      <td>0.8906</td>\n",
       "      <td>24577</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  model    dataset   modality  train_loss  train_f1  \\\n",
       "0   RecurrentSiameseNet  dataset_2       both      0.4996    0.7110   \n",
       "1   RecurrentSiameseNet  dataset_2   semantic      0.5842    0.6588   \n",
       "2   RecurrentSiameseNet  dataset_2  syntactic      0.5615    0.6761   \n",
       "3            SiameseNet  dataset_2       both      0.5632    0.6789   \n",
       "4            SiameseNet  dataset_2   semantic      0.4926    0.6880   \n",
       "5            SiameseNet  dataset_2  syntactic      0.5765    0.6589   \n",
       "6   RecurrentSiameseNet  dataset_3       both      0.6453    0.5927   \n",
       "7   RecurrentSiameseNet  dataset_3   semantic      0.6547    0.5506   \n",
       "8   RecurrentSiameseNet  dataset_3  syntactic      0.6689    0.5301   \n",
       "9            SiameseNet  dataset_3       both      0.6642    0.4908   \n",
       "10           SiameseNet  dataset_3   semantic      0.5596    0.7238   \n",
       "11           SiameseNet  dataset_3  syntactic      0.6873    0.4503   \n",
       "12  RecurrentSiameseNet  dataset_1       both      0.1305    0.9714   \n",
       "13  RecurrentSiameseNet  dataset_1   semantic      0.2850    0.9373   \n",
       "14  RecurrentSiameseNet  dataset_1  syntactic      0.2095    0.9529   \n",
       "15           SiameseNet  dataset_1       both      0.1839    0.9571   \n",
       "16           SiameseNet  dataset_1   semantic      0.0761    0.9821   \n",
       "17           SiameseNet  dataset_1  syntactic      0.2330    0.9477   \n",
       "\n",
       "    train_acc  test_loss  test_f1  test_acc  n_params  \n",
       "0      0.7344     0.5539   0.6496    0.6983     20641  \n",
       "1      0.6714     0.6066   0.6150    0.6501     18689  \n",
       "2      0.6911     0.5757   0.6486    0.6796    107137  \n",
       "3      0.6627     0.5719   0.6677    0.6652     73729  \n",
       "4      0.7502     0.6327   0.5726    0.6585     28865  \n",
       "5      0.6668     0.5829   0.6060    0.6406     24577  \n",
       "6      0.6248     0.6632   0.5774    0.6154    156289  \n",
       "7      0.6067     0.6731   0.5254    0.5801     18689  \n",
       "8      0.5927     0.6752   0.5051    0.5772    410881  \n",
       "9      0.5992     0.6668   0.4534    0.5826     32769  \n",
       "10     0.7170     0.7615   0.6039    0.5826     65921  \n",
       "11     0.5425     0.6900   0.4151    0.5223     81921  \n",
       "12     0.9488     0.2395   0.9475    0.9070    509185  \n",
       "13     0.8851     0.3680   0.9236    0.8600    148481  \n",
       "14     0.9163     0.2535   0.9492    0.9112      8353  \n",
       "15     0.9234     0.2548   0.9422    0.8973    180225  \n",
       "16     0.9692     0.2174   0.9496    0.9129     28865  \n",
       "17     0.9068     0.2764   0.9375    0.8906     24577  "
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def merge_splits(datasets):\n",
    "    \"\"\"merge train and valid splits\"\"\"\n",
    "    merged = {}\n",
    "    for idx, dataset in enumerate(datasets):\n",
    "        for problem_id in dataset.keys():\n",
    "            merged[str(idx) + '_' + problem_id] = dataset[problem_id]\n",
    "    return merged\n",
    "\n",
    "def test_seq():\n",
    "    results = []\n",
    "    torch.manual_seed(42)\n",
    "    with open('config.yaml', 'r') as f:\n",
    "        configs = yaml.load(f, Loader=yaml.FullLoader)\n",
    "    for (ds, ds_name), (model, batch_fn), modality in tqdm(get_combinations()):\n",
    "        batch_fn = partial(batch_fn, modality=modality)\n",
    "        config = configs[f'{ds_name}_{model.__name__}_{modality}']\n",
    "        train_data = merge_splits([ds['train'], ds['valid']])\n",
    "        test_data = ds['test']\n",
    "\n",
    "        if modality == 'both':\n",
    "            embed_dim = 384 + 61\n",
    "        elif modality == 'semantic':\n",
    "            embed_dim = 384\n",
    "        elif modality == 'syntactic':\n",
    "            embed_dim = 61\n",
    "        else:\n",
    "            raise ValueError(f'invalid modality {modality}')\n",
    "\n",
    "        model = model({'hidden_dim': config['hidden_dim'], 'dropout': config['dropout']}, embed_dim=embed_dim).to(device)\n",
    "        optimizer = torch.optim.AdamW(model.parameters(), lr=config['lr'])\n",
    "        train_batches = batch_fn(train_data, batch_size=config['batch_size'])\n",
    "        test_batches = batch_fn(test_data, batch_size=config['batch_size'])\n",
    "        train(model, optimizer, train_batches, batch_size=config['batch_size'], n_steps=config['n_steps'])\n",
    "        test_loss, test_f1, test_acc = evaluate_split(model, test_batches, steps=len(test_data) // config['batch_size'])\n",
    "        train_loss, train_f1, train_acc = evaluate_split(model, train_batches, steps=len(train_data) // config['batch_size'])\n",
    "        n_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "        result = {'model': model.__class__.__name__, 'dataset': ds_name, 'modality': modality,\n",
    "                  'train_loss': train_loss, 'train_f1': train_f1, 'train_acc': train_acc,\n",
    "                  'test_loss': test_loss, 'test_f1': test_f1, 'test_acc': test_acc,\n",
    "                'n_params': n_params}\n",
    "        results.append(result)\n",
    "    df = pd.DataFrame(results).round(4)\n",
    "    df.to_csv('results/true_siam_test_results.csv', index=False)\n",
    "    return df\n",
    "test_seq()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
