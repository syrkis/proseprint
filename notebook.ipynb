{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi Author Writing Style Analysis\n",
    "by: Noah Syrkis"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.utils import get_data, get_paired_dataset\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "import torch.nn.functional as F\n",
    "from collections import Counter, defaultdict\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import warnings; warnings.simplefilter('ignore')\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import xgboost as xgb\n",
    "from sklearn.metrics import accuracy_score, f1_score, confusion_matrix, precision_score, balanced_accuracy_score, roc_auc_score\n",
    "# make performance report\n",
    "from sklearn.metrics import classification_report\n",
    "from transformers import BertModel, BertTokenizer, BertForSequenceClassification, AdamW, get_linear_schedule_with_warmup\n",
    "import wandb\n",
    "api = wandb.Api()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# constants\n",
    "dataset_names = [f'pan23-multi-author-analysis-dataset{i}' for i in range(1, 4)]\n",
    "PAD = '<PAD>'  # id 0\n",
    "UNK = '<UNK>'  # id 1\n",
    "PAD_ID = 0\n",
    "UNK_ID = 1\n",
    "batch_size = 32\n",
    "block_size = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4200/4200 [00:03<00:00, 1355.93it/s]\n",
      "100%|██████████| 900/900 [00:00<00:00, 1252.40it/s]\n"
     ]
    }
   ],
   "source": [
    "# read in data\n",
    "one_train_data = get_data(dataset_names[0], 'train')\n",
    "one_valid_data = get_data(dataset_names[0], 'validation')\n",
    "# two_train_data = get_data(dataset_names[1], 'train')\n",
    "# two_valid_data = get_data(dataset_names[1], 'validation')\n",
    "# three_train_data = get_data(dataset_names[2], 'train')\n",
    "# three_valid_data = get_data(dataset_names[2], 'validation')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# functions\n",
    "encode = lambda x, stoi: [stoi.get(w, UNK_ID) for w in x]\n",
    "decode = lambda x, itos: ''.join([itos.get(i, UNK) for i in x])\n",
    "flatten = lambda x: [i for j in x for i in j]\n",
    "\n",
    "def make_word_vocab(dataset, min_count=50):\n",
    "    # make a vocabulary from the training set\n",
    "    vocab = Counter(flatten(flatten(dataset['text'])))\n",
    "    vocab = [w for w, c in vocab.most_common() if c > min_count] + ['\\n', ' '] + list('abcdefghijklmnopqrstuvwxyz') + list('ABCDEFGHIJKLMNOPQRSTUVWXYZ')\n",
    "    vocab = [PAD, UNK] + sorted(list(set(vocab)))\n",
    "    # return UNK if word is not in vocab\n",
    "    stoi = defaultdict(lambda: 1, {w: i for i, w in enumerate(vocab)})\n",
    "    itos = {i: w for i, w in enumerate(vocab)}\n",
    "    return stoi, itos\n",
    "\n",
    "\n",
    "def make_char_vocab(dataset, min_count=50):\n",
    "    # make a vocabulary from the training set\n",
    "    vocab = Counter(flatten(flatten(dataset['text'])))\n",
    "    vocab = [w for w, c in vocab.most_common() if c > min_count] + ['\\n', ' '] + list('abcdefghijklmnopqrstuvwxyz') + list('ABCDEFGHIJKLMNOPQRSTUVWXYZ')\n",
    "    vocab = [PAD, UNK] + sorted(list(set(vocab)))\n",
    "    # return UNK if word is not in vocab\n",
    "    stoi = defaultdict(lambda: 1, {w: i for i, w in enumerate(vocab)})\n",
    "    itos = {i: w for i, w in enumerate(vocab)}\n",
    "    return stoi, itos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "ctoi, itoc = make_char_vocab(one_train_data)  # character vocab\n",
    "wtoi, itow = make_word_vocab(one_train_data)  # word vocab"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baselines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_one_train_data = get_paired_dataset(one_train_data)\n",
    "base_one_valid_data = get_paired_dataset(one_valid_data)\n",
    "chars = [PAD, UNK] + list('abcdefghijklmnopqrstuvwxyz') + list('ABCDEFGHIJKLMNOPQRSTUVWXYZ') + ['\\n', ' ', '.', ',', '!', '?', ':', ';', '\"', \"'\", '(', ')', '-', '_', '/', '\\\\', '|', '[', ']', '{', '}', '@', '#', '$', '%', '^', '&', '*', '+', '=', '<', '>', '`', '~']\n",
    "ctoi = {c: i for i, c in enumerate(chars)}\n",
    "itoc = {i: c for i, c in enumerate(chars)}\n",
    "encode_base = lambda x: [ctoi.get(c, UNK_ID) for c in x]\n",
    "base_one_train_data[['doc1_idx', 'doc2_idx']] = base_one_train_data[['doc1', 'doc2']].applymap(encode_base)\n",
    "base_one_valid_data[['doc1_idx', 'doc2_idx']] = base_one_valid_data[['doc1', 'doc2']].applymap(encode_base)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf = TfidfVectorizer(max_features=200)\n",
    "tfidf.fit(base_one_train_data['doc1'] + base_one_train_data['doc2'])\n",
    "base_one_train_data['doc1_tfidf'] = tfidf.transform(base_one_train_data['doc1']).toarray().tolist()\n",
    "base_one_train_data['doc2_tfidf'] = tfidf.transform(base_one_train_data['doc2']).toarray().tolist()\n",
    "base_one_valid_data['doc1_tfidf'] = tfidf.transform(base_one_valid_data['doc1']).toarray().tolist()\n",
    "base_one_valid_data['doc2_tfidf'] = tfidf.transform(base_one_valid_data['doc2']).toarray().tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = np.array(base_one_train_data['doc1_tfidf'].tolist()) - np.array(base_one_train_data['doc2_tfidf'].tolist())\n",
    "y_train = base_one_train_data['change']\n",
    "X_valid = np.array(base_one_valid_data['doc1_tfidf'].tolist()) - np.array(base_one_valid_data['doc2_tfidf'].tolist())\n",
    "y_valid = base_one_valid_data['change']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "scale_pos_weight = y_train.value_counts()[0] / y_train.value_counts()[1]\n",
    "model = xgb.XGBClassifier(\n",
    "    scale_pos_weight=scale_pos_weight,\n",
    "    n_jobs=-1,\n",
    "    reg_alpha=0.1,\n",
    "    reg_lambda=10,\n",
    "    )\n",
    "model.fit(X_train, y_train)\n",
    "pred_train = model.predict(X_train)\n",
    "pred_valid = model.predict(X_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train XGBoost\n",
      "Weigthed F1: 0.9195433929494176\n",
      "\n",
      "Valid XGBoost\n",
      "Weigthed F1: 0.8235611190705914\n",
      "\n",
      "Train ones\n",
      "Weigthed F1: 0.8228830172296793\n",
      "\n",
      "Valid ones\n",
      "Weigthed F1: 0.8047955216326438\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def evaluate_predictions(name, y_true, y_pred):\n",
    "    print(name)\n",
    "    print('Weigthed F1:', f1_score(y_true, y_pred, average='weighted'))\n",
    "    print()\n",
    "\n",
    "evaluate_predictions('Train XGBoost', y_train, pred_train)\n",
    "evaluate_predictions('Valid XGBoost', y_valid, pred_valid)\n",
    "evaluate_predictions('Train ones', y_train, np.ones_like(y_train))\n",
    "evaluate_predictions('Valid ones', y_valid, np.ones_like(y_valid))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def evaluate(model, train_data, val_data, batch_size, prev_losses, batch_fn):\n",
    "    model.eval()\n",
    "    losses = {}\n",
    "    f1s = {}\n",
    "    for split, data in [('train', train_data), ('valid', val_data)]:\n",
    "        losses[split] = []\n",
    "        f1s[split] = []\n",
    "        for _ in range(10):\n",
    "            x1s, x2s, ys = batch_fn(data, batch_size)\n",
    "            ys_hat, loss = model(x1s, x2s, ys)\n",
    "            f1 = f1_score(ys.flatten().numpy(), ys_hat.argmax(1).flatten().numpy(), average='weighted')\n",
    "            losses[split].append(loss.item())\n",
    "            f1s[split].append(f1)\n",
    "        f1s[split] = np.mean(f1s[split])\n",
    "        losses[split] = np.mean(losses[split])\n",
    "    model.train()\n",
    "    if prev_losses is not None and prev_losses['valid'] > losses['valid'] and prev_losses['train'] > losses['train']:\n",
    "        torch.save(model.state_dict(), 'lm.pth')\n",
    "    return losses, f1s\n",
    "\n",
    "\n",
    "def train(model, train_data, valid_data, opt, conf, batch_fn, losses=None):\n",
    "    wandb.init(project='mawsa', entity='syrkis', config=conf)\n",
    "    # experiment.add_pytorch_models({'model': model})\n",
    "    for i in range(conf['n_iters']):\n",
    "        x1s, x2s, ys = batch_fn(train_data, conf['batch_size'])\n",
    "        _, loss = model(x1s, x2s, ys)\n",
    "        opt.zero_grad()\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "        if i % (conf['n_iters'] // 50) == 0:\n",
    "            losses, f1s = evaluate(model, train_data, valid_data, conf['batch_size'], losses, batch_fn)\n",
    "            wandb.log({'train_loss': losses['train'], 'valid_loss': losses['valid'], 'train_f1': f1s['train'], 'valid_f1': f1s['valid']})"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GRU"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Seperate samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = get_paired_dataset(one_train_data)\n",
    "valid_data = get_paired_dataset(one_valid_data)\n",
    "train_data['doc1'] = train_data['doc1'].apply(lambda x: encode(x, ctoi))\n",
    "train_data['doc2'] = train_data['doc2'].apply(lambda x: encode(x, ctoi))\n",
    "valid_data['doc1'] = valid_data['doc1'].apply(lambda x: encode(x, ctoi))\n",
    "valid_data['doc2'] = valid_data['doc2'].apply(lambda x: encode(x, ctoi))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_sequence(xs, max_len):\n",
    "    return torch.tensor([x + [PAD_ID] * (max_len - len(x)) for x in xs])\n",
    "\n",
    "def get_batch(data, batch_size):\n",
    "    idxs = np.random.choice(len(data), batch_size)\n",
    "    xs = data.iloc[idxs]\n",
    "    ys = xs['change'].values\n",
    "    xs = xs[['doc1', 'doc2']].values\n",
    "    max_len = max([len(x) for x in xs.flatten().tolist()])\n",
    "    xs = xs.flatten()\n",
    "    xs = pad_sequence(xs, max_len)\n",
    "    xs = xs.view(batch_size, 2, -1)\n",
    "    ys = torch.tensor(ys)\n",
    "    x1s = xs[:, 0, :]\n",
    "    x2s = xs[:, 1, :]\n",
    "    return x1s, x2s, ys\n",
    "# get_batch(train_data, 12).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GRUSiam(nn.Module):\n",
    "\n",
    "    def __init__(self, vocab_size, emb_size, hidden_size, n_layers, dropout):\n",
    "        super().__init__()\n",
    "        self.emb = nn.Embedding(vocab_size, emb_size)\n",
    "        self.gru = nn.GRU(emb_size, hidden_size, n_layers, batch_first=True, dropout=dropout)\n",
    "        self.fc = nn.Linear(hidden_size, 1)\n",
    "\n",
    "    def forward(self, x1, x2, y=None):\n",
    "        x1 = self.emb(x1)\n",
    "        x2 = self.emb(x2)\n",
    "        x1, _ = self.gru(x1)\n",
    "        x2, _ = self.gru(x2)\n",
    "        x1 = x1[:, -1, :]\n",
    "        x2 = x2[:, -1, :]\n",
    "        x = torch.abs(x1 - x2)\n",
    "        x = self.fc(x)\n",
    "        if y is not None:\n",
    "            loss = F.binary_cross_entropy_with_logits(x, y.view(-1, 1).float())\n",
    "        else:\n",
    "            loss = None\n",
    "        return x, loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:h7n6s199) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4db327216c32478e88cdb06162018f54",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.007 MB of 0.007 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>train_f1</td><td>▅▅▄▁▃▃█▄▃▃▄▃▃▄▅▄▄▃▃</td></tr><tr><td>train_loss</td><td>█▇▂▁▃▃▆▇▅▂▂▂▁▄▄▁▆▃▂</td></tr><tr><td>valid_f1</td><td>▁▃▃█▃▂▃▂▃▃▃▂▄▁▂▅▃▂▃</td></tr><tr><td>valid_loss</td><td>█▆▆▆▇▅▅▄▆▂▄▄▇▃▅▆▄▁█</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>train_f1</td><td>0.04</td></tr><tr><td>train_loss</td><td>0.55869</td></tr><tr><td>valid_f1</td><td>0.07</td></tr><tr><td>valid_loss</td><td>0.66059</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">logical-tree-15</strong> at: <a href='https://wandb.ai/syrkis/mawsa/runs/h7n6s199' target=\"_blank\">https://wandb.ai/syrkis/mawsa/runs/h7n6s199</a><br/>Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20230419_211408-h7n6s199/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:h7n6s199). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fa3f3da9ed724ad9accf3c20dd8ae350",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.01673384159997416, max=1.0)…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.14.2 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.13.11"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/Users/syrkis/Documents/ku/lp2/mawsa/wandb/run-20230419_212054-qg3orvag</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/syrkis/mawsa/runs/qg3orvag' target=\"_blank\">dry-snow-16</a></strong> to <a href='https://wandb.ai/syrkis/mawsa' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/syrkis/mawsa' target=\"_blank\">https://wandb.ai/syrkis/mawsa</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/syrkis/mawsa/runs/qg3orvag' target=\"_blank\">https://wandb.ai/syrkis/mawsa/runs/qg3orvag</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "conf = {'n_iters': 100, 'batch_size': 4, 'n_layers': 2, 'dropout': 0.1, 'emb_size': 32, 'hidden_size': 32}\n",
    "model = GRUSiam(len(ctoi), conf['emb_size'], conf['hidden_size'], conf['n_layers'], conf['dropout']) \n",
    "opt = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "train(model, train_data, valid_data, opt, conf, get_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
