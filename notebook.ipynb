{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi Author Writing Style Analysis\n",
    "\n",
    "The following notebook presents three different approaches to the problem of distingushing when in a sequence of paragraphs,\n",
    "the author changes. The first approach disregrads the order of the paragraphs, opting instead to view samples as pairs of paragraphs.\n",
    "It processes the paragraphs with a siamese network, which is a neural network that takes two inputs and outputs a single value.\n",
    "The second approach adds a recurrent layer to the siamese network, allowing it to take into account a sequence of paragraphs.\n",
    "The third approach builds on the second by augmenting the input with a manually engineered feature vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import pickle\n",
    "import torch\n",
    "import warnings\n",
    "from torch.nn import functional as F\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from tqdm import tqdm\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.metrics import f1_score\n",
    "import pandas as pd\n",
    "import time\n",
    "from src.utils import get_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "warnings.filterwarnings('ignore')\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## data\n",
    "\n",
    "We load the data (uncomment to reconstruct the data from the raw files),\n",
    "and make two data batch loaders:\n",
    "\n",
    "1. pairs of paragraphs, which will be used for our baseline siamese network.\n",
    "2. Sequences of paragraphs, which will be used for our recurrent siamese network.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data = { str(i): get_data(i) for i in range(1, 4) }\n",
    "# pickle.dump(data, open('data/data.pkl', 'wb'))\n",
    "data = pickle.load(open('data/data.pkl', 'rb'))\n",
    "dataset_1, dataset_2, dataset_3 = data['1'], data['2'], data['3']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def paired_samples(data_split):\n",
    "    \"\"\"turns data set into pair of consectuve sentences (flattens multi paragraph samples into pairs)\"\"\"\n",
    "    pairs = []\n",
    "    for problem_id in data_split.keys():\n",
    "        texts = data_split[problem_id]['text']\n",
    "        targets = data_split[problem_id]['truth']['changes']\n",
    "        if len(texts) - 1 != len(targets):\n",
    "            # TODO: fix. a few of the samples have more than one paragraph, making .readlines() wrong\n",
    "            # print(f'problem {problem_id} has {len(texts)} texts and {len(targets)} targets')\n",
    "            continue\n",
    "        for target, text1, text2 in zip(targets, texts[:-1], texts[1:]):\n",
    "            pairs.append((text1, text2, target))\n",
    "    random.shuffle(pairs)\n",
    "    return pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pair_batches(data_split, device, batch_size=32):\n",
    "    pairs = paired_samples(data_split)\n",
    "    while True:\n",
    "        # perm = np.random.permutation(len(pairs))\n",
    "        x1 = torch.tensor(np.array([p[0] for p in pairs])).to(device)\n",
    "        x2 = torch.tensor(np.array([p[1] for p in pairs])).to(device)\n",
    "        y = torch.tensor(np.array([p[2] for p in pairs])).to(device)\n",
    "        perm = torch.randperm(len(pairs))\n",
    "        for i in range(0, len(pairs), batch_size):\n",
    "            batch = perm[i:i+batch_size]\n",
    "            yield (x1[batch], x2[batch]), y[batch]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sequence_batches(data_split, device, batch_size=32):\n",
    "    \"\"\"turns data set into sequence of sentences (flattens multi paragraph samples into sequence)\"\"\"\n",
    "    x, y = [], []\n",
    "    for problem_id in data_split.keys():\n",
    "        texts = data_split[problem_id]['text']\n",
    "        targets = data_split[problem_id]['truth']['changes']\n",
    "        if len(texts) - 1 != len(targets):\n",
    "            continue\n",
    "        x.append(torch.tensor(texts))\n",
    "        y.append(torch.tensor(targets))\n",
    "    while True:\n",
    "        perm = torch.randperm(len(x))\n",
    "        for i in range(0, len(x), batch_size):\n",
    "            batch = perm[i:i+batch_size]\n",
    "            x_batch = [x[i] for i in batch]\n",
    "            y_batch = [y[i] for i in batch]\n",
    "            y_batch = torch.cat(y_batch, dim=0).to(device)\n",
    "            # pad with zero vectors\n",
    "            x_batch = pad_sequence(x_batch, batch_first=True, padding_value=0).to(device)\n",
    "            yield x_batch, y_batch"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## models\n",
    "\n",
    "We make our two models, the siamese network and the recurrent siamese network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SiameseNet(torch.nn.Module):\n",
    "    def __init__(self, config, embed_dim=384):\n",
    "        super().__init__()\n",
    "        self.embed_dim = embed_dim\n",
    "        self.hidden_dim = config['hidden_dim']\n",
    "        self.linear1 = torch.nn.Linear(self.embed_dim, self.hidden_dim)\n",
    "        self.linear2 = torch.nn.Linear(self.hidden_dim, self.hidden_dim)\n",
    "        self.linear3 = torch.nn.Linear(self.hidden_dim, 1)\n",
    "        self.dropout = torch.nn.Dropout(config['dropout'])\n",
    "        self.sigmoid = torch.nn.Sigmoid()\n",
    "    \n",
    "    def forward(self, x, y=None):\n",
    "        x1, x2 = x\n",
    "        x1_hat = self.dropout(x1)\n",
    "        x1_hat = self.linear1(x1_hat)\n",
    "        x1_hat = F.gelu(x1_hat)\n",
    "        x2_hat = self.dropout(x2)\n",
    "        x2_hat = self.linear1(x2_hat)\n",
    "        x2_hat = F.gelu(x2_hat)\n",
    "        y_hat = torch.abs(x1_hat - x2_hat)\n",
    "        y_hat = self.linear2(y_hat)\n",
    "        y_hat = F.gelu(y_hat)\n",
    "        y_hat = self.dropout(y_hat)\n",
    "        y_hat = self.linear3(y_hat)\n",
    "        y_hat = self.sigmoid(y_hat)\n",
    "        if y is not None:\n",
    "            loss = torch.nn.functional.binary_cross_entropy(y_hat, y.float().unsqueeze(1))\n",
    "            return y_hat, loss\n",
    "        return y_hat\n",
    "\n",
    "    def predict(self, x):\n",
    "        y_hat = self.forward(x)\n",
    "        y_hat = y_hat.squeeze(1)\n",
    "        y_hat = (y_hat > 0.5).int()\n",
    "        return y_hat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RecurrentSiameseNet(torch.nn.Module):\n",
    "    def __init__(self, config, embed_dim=384):\n",
    "        super().__init__()\n",
    "        self.embed_dim = embed_dim\n",
    "        self.hidden_dim = config['hidden_dim']\n",
    "        self.gru = torch.nn.GRU(self.embed_dim, self.hidden_dim, batch_first=True)\n",
    "        self.linear1 = torch.nn.Linear(self.hidden_dim, self.hidden_dim)\n",
    "        self.linear2 = torch.nn.Linear(self.hidden_dim, 1)\n",
    "        self.dropout = torch.nn.Dropout(config['dropout'])\n",
    "    \n",
    "    def forward(self, x, y=None):\n",
    "        # x is tensor of shape (batch_size, seq_len, embed_dim)\n",
    "        y_hat = self.dropout(x)\n",
    "        mask = self.x_mask(y_hat)\n",
    "        y_hat = self.gru(y_hat)[0]\n",
    "        y_hat = y_hat.reshape(-1, self.hidden_dim)  # flatten for masking and y_hat\n",
    "        y_hat = y_hat[mask]\n",
    "        y_hat = F.gelu(y_hat)\n",
    "        y_hat = self.linear1(y_hat)\n",
    "        y_hat = self.dropout(y_hat)\n",
    "        y_hat = F.gelu(y_hat)\n",
    "        y_hat = self.linear2(y_hat)\n",
    "        y_hat = F.sigmoid(y_hat)\n",
    "        y_hat = y_hat.view(-1)\n",
    "        if y is not None:\n",
    "            try: \n",
    "                loss = torch.nn.functional.binary_cross_entropy(y_hat, y.float())\n",
    "            except ValueError:\n",
    "                plt.imshow(x.cpu().numpy().sum(axis=2))\n",
    "                raise\n",
    "                \n",
    "            return y_hat, loss\n",
    "        return y_hat\n",
    "\n",
    "    def x_mask(self, x):\n",
    "        \"\"\"returns mask of shape (batch_size, seq_len)\"\"\"\n",
    "        mask = torch.sum(x, dim=2) != 0\n",
    "        mask[:, 0] = False\n",
    "        mask = mask.view(-1)\n",
    "        return mask\n",
    "\n",
    "    def predict(self, x):\n",
    "        y_hat = self.forward(x)\n",
    "        y_hat = (y_hat > 0.5).int()\n",
    "        return y_hat"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## train\n",
    "\n",
    "We define our training and evaluation functions, for use by both models, and all three datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_curve(metrics):\n",
    "    plt.style.use('dark_background')\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
    "    axes[0].plot(metrics['train_loss'], label='train')\n",
    "    axes[0].plot(metrics['valid_loss'], label='val')\n",
    "    axes[0].set_title('Loss')\n",
    "    axes[0].legend()\n",
    "    axes[1].plot(metrics['train_f1'], label='train')\n",
    "    axes[1].plot(metrics['valid_f1'], label='val')\n",
    "    axes[1].set_title('F1')\n",
    "    axes[1].legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(metrics, model, train_batches, valid_batches, steps=10):\n",
    "    for batch_name, batches in [('train', train_batches), ('valid', valid_batches)]:\n",
    "        loss, f1 = evaluate_split(model, batches, steps=steps)\n",
    "        metrics[batch_name + '_loss'].append(loss)\n",
    "        metrics[batch_name + '_f1'].append(f1)\n",
    "        model.eval()\n",
    "    model.train()\n",
    "    return metrics\n",
    "\n",
    "def evaluate_split(model, batches, steps):\n",
    "    f1_scores, losses = [], []\n",
    "    for i in range(steps):\n",
    "        x, y = next(batches)\n",
    "        y_hat, loss = model(x, y)\n",
    "        losses.append(loss.item())\n",
    "        y_hat = model.predict(x).cpu().numpy().astype(int)\n",
    "        y = y.cpu().numpy().astype(int)\n",
    "        f1_scores.append(f1_score(y, y_hat))\n",
    "    return np.mean(losses), np.mean(f1_scores)\n",
    "\n",
    "def train(model, optimizer, train_batches, valid_batches, n_steps):\n",
    "    metrics = {'train_loss': [], 'train_f1': [], 'valid_loss': [], 'valid_f1': []}\n",
    "    for i in range(n_steps):\n",
    "        x, y = next(train_batches)\n",
    "        y_hat, loss = model(x, y)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if i % (n_steps // 100) == 0:\n",
    "            metrics = evaluate(metrics, model, train_batches, valid_batches)\n",
    "    final = evaluate(metrics, model, train_batches, valid_batches, steps=100)\n",
    "    return metrics, {k: v[-1] for k, v in final.items()}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### experiment\n",
    "\n",
    "To do hyper paramter tuning, and test performance of our models on our three datasets, we define the experiment functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hyper_params():\n",
    "    \"\"\"return random hyperparameters\"\"\"\n",
    "    return {\n",
    "        'lr': 10 ** random.uniform(-5, -2),\n",
    "        'dropout': random.uniform(0, 0.5),\n",
    "        'hidden_dim': random.randint(64, 256),\n",
    "        'batch_size': [16, 32, 64, 128][random.randint(0, 2)],\n",
    "        'n_steps': [1000, 2000, 3000, 4000, 5000, 6000, 7000][random.randint(0, 2)]\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hyper_param_search(model_fn, batch_fn, dataset, n_trials=10):\n",
    "    \"\"\"search hyperparameters for a given model and dataset\"\"\"\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    hyper_param_metrics = []\n",
    "    for i in tqdm(range(n_trials)):\n",
    "        config = hyper_params()\n",
    "        model = model_fn(config).to(device)\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=config['lr'])\n",
    "        train_batches = batch_fn(dataset['train'], device, config['batch_size'])\n",
    "        valid_batches = batch_fn(dataset['valid'], device, config['batch_size'])\n",
    "        _, final= train(model, optimizer, train_batches, valid_batches, config['n_steps'])\n",
    "        hyper_param_metrics.append({**config, **final})\n",
    "    df = pd.DataFrame(hyper_param_metrics).sort_values('valid_f1', ascending=False)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def experiment(n_trials=10):\n",
    "    dfs = []\n",
    "    for ds, ds_name in [(dataset_2, 'dataset_2'), (dataset_3, 'dataset_3')]:\n",
    "        runs = [(RecurrentSiameseNet, get_sequence_batches, ds), (SiameseNet, get_pair_batches, ds)]\n",
    "        for model, batch_fn, ds in runs:\n",
    "            print(f'running {model.__name__}' + f\" on {ds_name}\")\n",
    "            df = hyper_param_search(model, batch_fn, ds, n_trials)\n",
    "            file_name = f'results/{model.__name__}_{ds_name}.csv'\n",
    "            df.to_csv(file_name)\n",
    "            dfs.append(df)\n",
    "    return dfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running RecurrentSiameseNet on dataset_2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 29%|██▉       | 29/100 [07:19<16:52, 14.26s/it]"
     ]
    }
   ],
   "source": [
    "dfs = experiment(100)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
