{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi Author Writing Style Analysis\n",
    "\n",
    "The following notebook presents three different approaches to the problem of distingushing when in a sequence of paragraphs,\n",
    "the author changes. The first approach disregrads the order of the paragraphs, opting instead to view samples as pairs of paragraphs.\n",
    "It processes the paragraphs with a siamese network, which is a neural network that takes two inputs and outputs a single value.\n",
    "The second approach adds a recurrent layer to the siamese network, allowing it to take into account a sequence of paragraphs.\n",
    "The third approach builds on the second by augmenting the input with a manually engineered feature vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import pickle\n",
    "import torch\n",
    "import warnings\n",
    "from functools import partial\n",
    "from torch.nn import functional as F\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "import itertools\n",
    "from tqdm import tqdm\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.metrics import f1_score, accuracy_score\n",
    "import pandas as pd\n",
    "import nltk\n",
    "import yaml\n",
    "from src.utils import get_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "warnings.filterwarnings('ignore')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data\n",
    "\n",
    "We load the data (uncomment to reconstruct the data from the raw files),\n",
    "and make two data batch loaders:\n",
    "\n",
    "1. pairs of paragraphs, which will be used for our baseline siamese network.\n",
    "2. Sequences of paragraphs, which will be used for our recurrent siamese network.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nltk.download('averaged_perceptron_tagger')\n",
    "# data = { str(i): get_data(i) for i in range(1, 4) }\n",
    "# pickle.dump(data, open('data/data.pkl', 'wb'))\n",
    "data = pickle.load(open('data/data.pkl', 'rb'))\n",
    "dataset_1, dataset_2, dataset_3 = data['1'], data['2'], data['3']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "def paired_samples(data_split, syntactic_flag):\n",
    "    \"\"\"turns data set into pair of consectuve sentences (flattens multi paragraph samples into pairs)\"\"\"\n",
    "    pairs = []\n",
    "    for problem_id in data_split.keys():\n",
    "        semantic = data_split[problem_id]['semantic']\n",
    "        syntactic = data_split[problem_id]['syntactic']\n",
    "        # concatenate all symantic and syntactic features into one vecntor per sample\n",
    "        if syntactic_flag:\n",
    "            texts = [np.concatenate([semantic[i], syntactic[i]]) for i in range(len(semantic))]\n",
    "        else:\n",
    "            texts = data_split[problem_id]['semantic']\n",
    "        targets = data_split[problem_id]['truth']['changes']\n",
    "        if len(texts) - 1 != len(targets):\n",
    "            # TODO: fix. a few of the samples have more than one paragraph, making .readlines() wrong\n",
    "            # print(f'problem {problem_id} has {len(texts)} texts and {len(targets)} targets')\n",
    "            continue\n",
    "        for target, text1, text2 in zip(targets, texts[:-1], texts[1:]):\n",
    "            pairs.append((text1, text2, target))\n",
    "    random.shuffle(pairs)\n",
    "    return pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pair_batches(data_split, syntactic_flag=True, batch_size=32):\n",
    "    pairs = paired_samples(data_split, syntactic_flag)\n",
    "    while True:\n",
    "        # perm = np.random.permutation(len(pairs))\n",
    "        x1 = torch.tensor(np.array([p[0] for p in pairs])).float().to(device)\n",
    "        x2 = torch.tensor(np.array([p[1] for p in pairs])).float().to(device)\n",
    "        y = torch.tensor(np.array([p[2] for p in pairs])).float().to(device)\n",
    "        perm = torch.randperm(len(pairs))\n",
    "        for i in range(0, len(pairs), batch_size):\n",
    "            batch = perm[i:i+batch_size]\n",
    "            yield (x1[batch], x2[batch]), y[batch]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sequence_batches(data_split, syntactic_flag=True, batch_size=32):\n",
    "    \"\"\"turns data set into sequence of sentences (flattens multi paragraph samples into sequence)\"\"\"\n",
    "    x, y = [], []\n",
    "    for problem_id in data_split.keys():\n",
    "        semantic = data_split[problem_id]['semantic']\n",
    "        syntactic = data_split[problem_id]['syntactic']\n",
    "        # concatenate all symantic and syntactic features into one vecntor per sample\n",
    "        if syntactic_flag:\n",
    "            texts = [np.concatenate([semantic[i], syntactic[i]]) for i in range(len(semantic))]\n",
    "        else:\n",
    "            texts = data_split[problem_id]['semantic']\n",
    "        targets = data_split[problem_id]['truth']['changes']\n",
    "        if len(texts) - 1 != len(targets):\n",
    "            continue\n",
    "        x.append(torch.tensor(texts))\n",
    "        y.append(torch.tensor(targets))\n",
    "    while True:\n",
    "        perm = torch.randperm(len(x))\n",
    "        for i in range(0, len(x), batch_size):\n",
    "            batch = perm[i:i+batch_size]\n",
    "            x_batch = [x[i] for i in batch]\n",
    "            y_batch = [y[i] for i in batch]\n",
    "            y_batch = torch.cat(y_batch, dim=0).to(device)\n",
    "            # pad with zero vectors\n",
    "            x_batch = pad_sequence(x_batch, batch_first=True, padding_value=0).to(device)\n",
    "            x_batch = x_batch.float()\n",
    "            yield x_batch, y_batch"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Models\n",
    "\n",
    "We make our two models, the siamese network and the recurrent siamese network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SiameseNet(torch.nn.Module):\n",
    "    def __init__(self, config, embed_dim):\n",
    "        super().__init__()\n",
    "        self.embed_dim = embed_dim\n",
    "        self.hidden_dim = config['hidden_dim']\n",
    "        self.linear1 = torch.nn.Linear(self.embed_dim, self.hidden_dim)\n",
    "        self.linear2 = torch.nn.Linear(self.hidden_dim, self.hidden_dim)\n",
    "        self.linear3 = torch.nn.Linear(self.hidden_dim, 1)\n",
    "        self.dropout = torch.nn.Dropout(config['dropout'])\n",
    "        self.sigmoid = torch.nn.Sigmoid()\n",
    "    \n",
    "    def forward(self, x, y=None):\n",
    "        x1, x2 = x\n",
    "        x1_hat = self.dropout(x1)\n",
    "        x1_hat = self.linear1(x1_hat)\n",
    "        x1_hat = F.gelu(x1_hat)\n",
    "        x2_hat = self.dropout(x2)\n",
    "        x2_hat = self.linear1(x2_hat)\n",
    "        x2_hat = F.gelu(x2_hat)\n",
    "        y_hat = torch.abs(x1_hat - x2_hat)\n",
    "        y_hat = self.linear2(y_hat)\n",
    "        y_hat = F.gelu(y_hat)\n",
    "        y_hat = self.dropout(y_hat)\n",
    "        y_hat = self.linear3(y_hat)\n",
    "        y_hat = self.sigmoid(y_hat)\n",
    "        if y is not None:\n",
    "            loss = torch.nn.functional.binary_cross_entropy(y_hat, y.float().unsqueeze(1))\n",
    "            return y_hat, loss\n",
    "        return y_hat\n",
    "\n",
    "    def predict(self, x):\n",
    "        y_hat = self.forward(x)\n",
    "        y_hat = y_hat.squeeze(1)\n",
    "        y_hat = (y_hat > 0.5).int()\n",
    "        return y_hat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RecurrentSiameseNet(torch.nn.Module):\n",
    "    def __init__(self, config, embed_dim):\n",
    "        super().__init__()\n",
    "        self.embed_dim = embed_dim\n",
    "        self.hidden_dim = config['hidden_dim']\n",
    "        self.linear1 = torch.nn.Linear(self.embed_dim, self.hidden_dim)\n",
    "        self.gru = torch.nn.GRU(self.hidden_dim, self.hidden_dim, batch_first=True)\n",
    "        self.linear2 = torch.nn.Linear(self.hidden_dim, 1)\n",
    "        self.dropout = torch.nn.Dropout(config['dropout'])\n",
    "    \n",
    "    def forward(self, x, y=None):\n",
    "        # x is tensor of shape (batch_size, seq_len, embed_dim)\n",
    "        mask = self.x_mask(x)\n",
    "        y_hat = self.dropout(x)\n",
    "        y_hat = y_hat.reshape(-1, self.embed_dim)\n",
    "        y_hat = self.linear1(y_hat) \n",
    "        y_hat = F.gelu(y_hat)\n",
    "        y_hat = y_hat.reshape(-1, x.shape[1], self.hidden_dim)\n",
    "        y_hat = self.gru(y_hat)[0]\n",
    "        y_hat = y_hat.reshape(-1, self.hidden_dim)  # flatten for masking and y_hat\n",
    "        y_hat = y_hat[mask]\n",
    "        y_hat = F.gelu(y_hat)\n",
    "        y_hat = self.linear2(y_hat)\n",
    "        y_hat = F.sigmoid(y_hat)\n",
    "        y_hat = y_hat.view(-1)\n",
    "        if y is not None:\n",
    "            try: \n",
    "                loss = torch.nn.functional.binary_cross_entropy(y_hat, y.float())\n",
    "            except ValueError:\n",
    "                return y_hat, None \n",
    "            return y_hat, loss\n",
    "        return y_hat\n",
    "\n",
    "    def x_mask(self, x):\n",
    "        \"\"\"returns mask of shape (batch_size, seq_len)\"\"\"\n",
    "        mask = torch.sum(x, dim=2) != 0\n",
    "        mask[:, 0] = False\n",
    "        mask = mask.view(-1)\n",
    "        return mask\n",
    "\n",
    "    def predict(self, x):\n",
    "        y_hat = self.forward(x)\n",
    "        y_hat = (y_hat > 0.5).int()\n",
    "        return y_hat"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training\n",
    "\n",
    "We define our training and evaluation functions, for use by both models, and all three datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_curve(metrics):\n",
    "    plt.style.use('dark_background')\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(16, 4))\n",
    "    axes[0].plot(metrics['train_loss'], label='train')\n",
    "    axes[0].plot(metrics['valid_loss'], label='val')\n",
    "    axes[0].set_title('Loss')\n",
    "    axes[0].legend()\n",
    "    axes[1].plot(metrics['train_f1'], label='train')\n",
    "    axes[1].plot(metrics['valid_f1'], label='val')\n",
    "    axes[1].set_title('F1')\n",
    "    axes[1].legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(metrics, model, train_batches, valid_batches, steps=10):\n",
    "    for batch_name, batches in [('train', train_batches), ('valid', valid_batches)]:\n",
    "        loss, f1, acc = evaluate_split(model, batches, steps=steps)\n",
    "        metrics[batch_name + '_loss'].append(loss)\n",
    "        metrics[batch_name + '_f1'].append(f1)\n",
    "        metrics[batch_name + '_acc'].append(acc)\n",
    "        model.eval()\n",
    "    model.train()\n",
    "    return metrics\n",
    "\n",
    "def evaluate_split(model, batches, steps):\n",
    "    f1_scores, losses, acc_scores = [], [], []\n",
    "    for i in range(steps):\n",
    "        x, y = next(batches)\n",
    "        y_hat, loss = model(x, y)\n",
    "        if loss is None:\n",
    "            continue\n",
    "        losses.append(loss.item())\n",
    "        y_hat = model.predict(x).cpu().numpy().astype(int)\n",
    "        y = y.cpu().numpy().astype(int)\n",
    "        f1_scores.append(f1_score(y, y_hat))\n",
    "        acc_scores.append(accuracy_score(y, y_hat))\n",
    "    return np.mean(losses), np.mean(f1_scores), np.mean(acc_scores)\n",
    "\n",
    "def train(model, optimizer, train_batches, valid_batches=None, batch_size=32, n_steps=1000):\n",
    "    # returns metrics and final scores, if doing validation, else returns final model for testing\n",
    "    metrics = {'train_loss': [], 'train_f1': [], 'train_acc': [], 'valid_loss': [], 'valid_f1': [], 'valid_acc': []}\n",
    "    for i in range(n_steps):\n",
    "        x, y = next(train_batches)\n",
    "        y_hat, loss = model(x, y)\n",
    "        if loss is None:  # there is an extremly rare bug where y_hat is one short of y FIXME.\n",
    "            continue\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if valid_batches and i % (n_steps // 100) == 0:\n",
    "            metrics = evaluate(metrics, model, train_batches, valid_batches)\n",
    "    if valid_batches:\n",
    "        final = evaluate(metrics, model, train_batches, valid_batches, steps=4200 // batch_size)\n",
    "        return metrics, {k: v[-1] for k, v in final.items()}\n",
    "    return model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment\n",
    "\n",
    "To do hyper paramter tuning, and test performance of our models on our three datasets, we define the experiment functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hyper_params():\n",
    "    \"\"\"return random hyperparameters\"\"\"\n",
    "    return {\n",
    "        'lr': 10 ** random.choice([-3, -4, -5]),\n",
    "        'dropout': random.choice([0.1, 0.2, 0.3]),\n",
    "        'hidden_dim': random.choice([32, 64, 128, 256]),\n",
    "        'batch_size': random.choice([16, 32, 64]),\n",
    "        'n_steps': random.choice([1000, 2000, 4000, 6000, 8000, 10000]),\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hyper_param_search(model_fn, batch_fn, dataset, syntac_bool, n_trials=10):\n",
    "    \"\"\"search hyperparameters for a given model and dataset\"\"\"\n",
    "    embed_dim = 384 + 61 if syntac_bool else 384\n",
    "    hyper_param_metrics = []\n",
    "    training_metrics_list = []\n",
    "    for i in tqdm(range(n_trials)):\n",
    "        config = hyper_params()\n",
    "        model = model_fn(config, embed_dim=embed_dim).to(device)\n",
    "        optimizer = torch.optim.AdamW(model.parameters(), lr=config['lr'])\n",
    "        train_batches = batch_fn(dataset['train'], batch_size=config['batch_size'])\n",
    "        valid_batches = batch_fn(dataset['valid'], batch_size=config['batch_size'])\n",
    "        training_metrics, final= train(model, optimizer, train_batches, valid_batches, config['batch_size'], config['n_steps'])\n",
    "        training_metrics_list.append(training_metrics)\n",
    "        hyper_param_metrics.append({**config, **final})\n",
    "    df = pd.DataFrame(hyper_param_metrics).sort_values('valid_f1', ascending=False)\n",
    "    return df, training_metrics_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_combinations():\n",
    "    datasets = [(dataset_1, 'dataset_1'), (dataset_2, 'dataset_2'), (dataset_3, 'dataset_3')]\n",
    "    models = [(SiameseNet, get_pair_batches), (RecurrentSiameseNet, get_sequence_batches)]\n",
    "    syntac_bools = [True, False]\n",
    "    return list(itertools.product(datasets, models, syntac_bools))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "def experiment(n_trials=10):\n",
    "    for (ds, ds_name), (model, batch_fn), syntax_bool in get_combinations():\n",
    "        print(f'{model.__name__} on {ds_name} with {\"syntax\" if syntax_bool else \"no syntax\"}')\n",
    "        batch_fn = partial(batch_fn, syntactic_flag=syntax_bool)\n",
    "        df, training_metrics_list = hyper_param_search(model, batch_fn, ds, syntax_bool, n_trials=n_trials).round(4)\n",
    "        df.to_csv(df_file_name = f'results/{model.__name__}_{ds_name}_{\"syntax\" if syntax_bool else \"no_syntax\"}.csv', index=False)\n",
    "        pickle.dump(training_metrics_list, open(f'results/training_metrics_{model.__name__}_{ds_name}_{\"syntax\" if syntax_bool else \"no_syntax\"}.pkl', 'wb'))\n",
    "        print(f'best f1 score: {df.iloc[0][\"valid_f1\"]}')\n",
    "        print(f'best accuracy score: {df.iloc[0][\"valid_acc\"]}')\n",
    "        print()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SiameseNet on dataset_1 with syntax\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 12/30 [02:06<03:12, 10.71s/it]"
     ]
    }
   ],
   "source": [
    "experiment(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dfs():\n",
    "    dfs = {}\n",
    "    for ds_name in ['dataset_1', 'dataset_2', 'dataset_3']:\n",
    "        for model in [RecurrentSiameseNet, SiameseNet]:\n",
    "            file_name = f'results/{model.__name__}_{ds_name}.csv'\n",
    "            df = pd.read_csv(file_name)\n",
    "            dfs[(model.__name__, ds_name)] = df\n",
    "    return dfs"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 12/12 [01:00<00:00,  5.03s/it]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>dataset</th>\n",
       "      <th>syntax</th>\n",
       "      <th>loss</th>\n",
       "      <th>f1</th>\n",
       "      <th>acc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>SiameseNet</td>\n",
       "      <td>dataset_1</td>\n",
       "      <td>True</td>\n",
       "      <td>0.349138</td>\n",
       "      <td>0.921826</td>\n",
       "      <td>0.857143</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>SiameseNet</td>\n",
       "      <td>dataset_1</td>\n",
       "      <td>False</td>\n",
       "      <td>0.292061</td>\n",
       "      <td>0.933819</td>\n",
       "      <td>0.877232</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>RecurrentSiameseNet</td>\n",
       "      <td>dataset_1</td>\n",
       "      <td>True</td>\n",
       "      <td>0.305585</td>\n",
       "      <td>0.934038</td>\n",
       "      <td>0.878506</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>RecurrentSiameseNet</td>\n",
       "      <td>dataset_1</td>\n",
       "      <td>False</td>\n",
       "      <td>0.363395</td>\n",
       "      <td>0.926201</td>\n",
       "      <td>0.863879</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>SiameseNet</td>\n",
       "      <td>dataset_2</td>\n",
       "      <td>True</td>\n",
       "      <td>0.686908</td>\n",
       "      <td>0.540626</td>\n",
       "      <td>0.598214</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>SiameseNet</td>\n",
       "      <td>dataset_2</td>\n",
       "      <td>False</td>\n",
       "      <td>0.648064</td>\n",
       "      <td>0.425244</td>\n",
       "      <td>0.598214</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>RecurrentSiameseNet</td>\n",
       "      <td>dataset_2</td>\n",
       "      <td>True</td>\n",
       "      <td>0.597997</td>\n",
       "      <td>0.592719</td>\n",
       "      <td>0.669383</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>RecurrentSiameseNet</td>\n",
       "      <td>dataset_2</td>\n",
       "      <td>False</td>\n",
       "      <td>0.623673</td>\n",
       "      <td>0.589662</td>\n",
       "      <td>0.657867</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>SiameseNet</td>\n",
       "      <td>dataset_3</td>\n",
       "      <td>True</td>\n",
       "      <td>0.684411</td>\n",
       "      <td>0.262709</td>\n",
       "      <td>0.533482</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>SiameseNet</td>\n",
       "      <td>dataset_3</td>\n",
       "      <td>False</td>\n",
       "      <td>0.690813</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.517857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>RecurrentSiameseNet</td>\n",
       "      <td>dataset_3</td>\n",
       "      <td>True</td>\n",
       "      <td>0.674411</td>\n",
       "      <td>0.496191</td>\n",
       "      <td>0.576307</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>RecurrentSiameseNet</td>\n",
       "      <td>dataset_3</td>\n",
       "      <td>False</td>\n",
       "      <td>0.673394</td>\n",
       "      <td>0.526492</td>\n",
       "      <td>0.589374</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  model    dataset  syntax      loss        f1       acc\n",
       "0            SiameseNet  dataset_1    True  0.349138  0.921826  0.857143\n",
       "1            SiameseNet  dataset_1   False  0.292061  0.933819  0.877232\n",
       "2   RecurrentSiameseNet  dataset_1    True  0.305585  0.934038  0.878506\n",
       "3   RecurrentSiameseNet  dataset_1   False  0.363395  0.926201  0.863879\n",
       "4            SiameseNet  dataset_2    True  0.686908  0.540626  0.598214\n",
       "5            SiameseNet  dataset_2   False  0.648064  0.425244  0.598214\n",
       "6   RecurrentSiameseNet  dataset_2    True  0.597997  0.592719  0.669383\n",
       "7   RecurrentSiameseNet  dataset_2   False  0.623673  0.589662  0.657867\n",
       "8            SiameseNet  dataset_3    True  0.684411  0.262709  0.533482\n",
       "9            SiameseNet  dataset_3   False  0.690813  0.000000  0.517857\n",
       "10  RecurrentSiameseNet  dataset_3    True  0.674411  0.496191  0.576307\n",
       "11  RecurrentSiameseNet  dataset_3   False  0.673394  0.526492  0.589374"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def merge_splits(datasets):\n",
    "    \"\"\"merge train and valid splits\"\"\"\n",
    "    merged = {}\n",
    "    for dataset in datasets:\n",
    "        for problem_id in dataset.keys():\n",
    "            merged[problem_id] = dataset[problem_id]\n",
    "    return merged\n",
    "\n",
    "def test_seq():\n",
    "    results = []\n",
    "    with open('config.yaml', 'r') as f:\n",
    "        configs = yaml.load(f, Loader=yaml.FullLoader)\n",
    "    for (ds, ds_name), (model, batch_fn), syntax_bool in tqdm(get_combinations()):\n",
    "        batch_fn = partial(batch_fn, syntactic_flag=syntax_bool)\n",
    "        config = configs[ds_name]['syntactic' if syntax_bool else 'semantic']['siamese' if model== SiameseNet else 'recurrent']\n",
    "        train_data = merge_splits([ds['train'], ds['valid']])\n",
    "        test_data = ds['test']\n",
    "        embed_dim = 384 + 61 if syntax_bool else 384\n",
    "        model = model({'hidden_dim': config['hidden_dim'], 'dropout': config['dropout']}, embed_dim=embed_dim).to(device)\n",
    "        optimizer = torch.optim.AdamW(model.parameters(), lr=config['lr'])\n",
    "        train_batches = batch_fn(train_data, batch_size=config['batch_size'])\n",
    "        test_batches = batch_fn(test_data, batch_size=config['batch_size'])\n",
    "        train(model, optimizer, train_batches, batch_size=config['batch_size'], n_steps=config['n_steps'])\n",
    "        loss, f1, acc = evaluate_split(model, test_batches, steps=len(test_data) // config['batch_size'])\n",
    "        result = {'model': model.__class__.__name__, 'dataset': ds_name, 'syntax': syntax_bool, 'loss': loss, 'f1': f1, 'acc': acc}\n",
    "        results.append(result)\n",
    "    df = pd.DataFrame(results).round(4)\n",
    "    df.to_csv('results/test_results.csv', index=False)\n",
    "    return df\n",
    "\n",
    "test_seq()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
