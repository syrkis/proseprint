{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi Author Writing Style Analysis\n",
    "\n",
    "The following notebook presents three different approaches to the problem of distingushing when in a sequence of paragraphs,\n",
    "the author changes. The first approach disregrads the order of the paragraphs, opting instead to view samples as pairs of paragraphs.\n",
    "It processes the paragraphs with a siamese network, which is a neural network that takes two inputs and outputs a single value.\n",
    "The second approach adds a recurrent layer to the siamese network, allowing it to take into account a sequence of paragraphs.\n",
    "The third approach builds on the second by augmenting the input with a manually engineered feature vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import pickle\n",
    "import torch\n",
    "import warnings\n",
    "from torch.nn import functional as F\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from tqdm import tqdm\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.metrics import f1_score\n",
    "import pandas as pd\n",
    "from src.utils import get_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "warnings.filterwarnings('ignore')\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data = { str(i): get_data(i) for i in range(1, 4) }\n",
    "# pickle.dump(data, open('data/data.pkl', 'wb'))\n",
    "data = pickle.load(open('data/data.pkl', 'rb'))\n",
    "dataset_1, dataset_2, dataset_3 = data['1'], data['2'], data['3']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def paired_samples(data_split):\n",
    "    \"\"\"turns data set into pair of consectuve sentences (flattens multi paragraph samples into pairs)\"\"\"\n",
    "    pairs = []\n",
    "    for problem_id in data_split.keys():\n",
    "        texts = data_split[problem_id]['text']\n",
    "        targets = data_split[problem_id]['truth']['changes']\n",
    "        if len(texts) - 1 != len(targets):\n",
    "            # TODO: fix. a few of the samples have more than one paragraph, making .readlines() wrong\n",
    "            # print(f'problem {problem_id} has {len(texts)} texts and {len(targets)} targets')\n",
    "            continue\n",
    "        for target, text1, text2 in zip(targets, texts[:-1], texts[1:]):\n",
    "            pairs.append((text1, text2, target))\n",
    "    random.shuffle(pairs)\n",
    "    return pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pair_batches(data_split, device, batch_size=32):\n",
    "    pairs = paired_samples(data_split)\n",
    "    while True:\n",
    "        # perm = np.random.permutation(len(pairs))\n",
    "        x1 = torch.tensor(np.array([p[0] for p in pairs])).to(device)\n",
    "        x2 = torch.tensor(np.array([p[1] for p in pairs])).to(device)\n",
    "        y = torch.tensor(np.array([p[2] for p in pairs])).to(device)\n",
    "        perm = torch.randperm(len(pairs))\n",
    "        for i in range(0, len(pairs), batch_size):\n",
    "            batch = perm[i:i+batch_size]\n",
    "            yield (x1[batch], x2[batch]), y[batch]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sequence_batches(data_split, device, batch_size=32):\n",
    "    \"\"\"turns data set into sequence of sentences (flattens multi paragraph samples into sequence)\"\"\"\n",
    "    x, y = [], []\n",
    "    for problem_id in data_split.keys():\n",
    "        texts = data_split[problem_id]['text']\n",
    "        targets = data_split[problem_id]['truth']['changes']\n",
    "        if len(texts) - 1 != len(targets):\n",
    "            continue\n",
    "        x.append(torch.tensor(texts))\n",
    "        y.append(torch.tensor(targets))\n",
    "    while True:\n",
    "        perm = torch.randperm(len(x))\n",
    "        for i in range(0, len(x), batch_size):\n",
    "            batch = perm[i:i+batch_size]\n",
    "            x_batch = [x[i] for i in batch]\n",
    "            y_batch = [y[i] for i in batch]\n",
    "            y_batch = torch.cat(y_batch, dim=0).to(device)\n",
    "            # pad with zero vectors\n",
    "            x_batch = pad_sequence(x_batch, batch_first=True, padding_value=0).to(device)\n",
    "            yield x_batch, y_batch"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SiameseNet(torch.nn.Module):\n",
    "    def __init__(self, config, embed_dim=384):\n",
    "        super().__init__()\n",
    "        self.embed_dim = embed_dim\n",
    "        self.hidden_dim = config['hidden_dim']\n",
    "        self.dropout = config['dropout']\n",
    "        self.linear1 = torch.nn.Linear(self.embed_dim, self.hidden_dim)\n",
    "        self.linear2 = torch.nn.Linear(self.hidden_dim, self.hidden_dim)\n",
    "        self.linear3 = torch.nn.Linear(self.hidden_dim, 1)\n",
    "        self.dropout = torch.nn.Dropout(self.dropout)\n",
    "        self.sigmoid = torch.nn.Sigmoid()\n",
    "    \n",
    "    def forward(self, x, y=None):\n",
    "        x1, x2 = x\n",
    "        x1_hat = self.dropout(x1)\n",
    "        x1_hat = self.linear1(x1_hat)\n",
    "        x1_hat = F.gelu(x1_hat)\n",
    "        x2_hat = self.dropout(x2)\n",
    "        x2_hat = self.linear1(x2_hat)\n",
    "        x2_hat = F.gelu(x2_hat)\n",
    "        y_hat = torch.abs(x1_hat - x2_hat)\n",
    "        y_hat = self.linear2(y_hat)\n",
    "        y_hat = F.gelu(y_hat)\n",
    "        y_hat = self.dropout(y_hat)\n",
    "        y_hat = self.linear3(y_hat)\n",
    "        y_hat = self.sigmoid(y_hat)\n",
    "        if y is not None:\n",
    "            loss = torch.nn.functional.binary_cross_entropy(y_hat, y.float().unsqueeze(1))\n",
    "            return y_hat, loss\n",
    "        return y_hat\n",
    "\n",
    "    def predict(self, x):\n",
    "        y_hat = self.forward(x)\n",
    "        y_hat = y_hat.squeeze(1)\n",
    "        y_hat = (y_hat > 0.5).int()\n",
    "        return y_hat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RecurrentSiameseNet(torch.nn.Module):\n",
    "    def __init__(self, config, embed_dim=384):\n",
    "        super().__init__()\n",
    "        self.hidden_dim = config['hidden_dim']\n",
    "        self.embed_dim = embed_dim\n",
    "        self.gru = torch.nn.GRU(self.embed_dim, self.hidden_dim, batch_first=True)\n",
    "        self.linear1 = torch.nn.Linear(self.hidden_dim, 1)\n",
    "    \n",
    "    def forward(self, x, y=None):\n",
    "        # x is tensor of shape (batch_size, seq_len, embed_dim)\n",
    "        mask = self.x_mask(x)\n",
    "        x = self.gru(x)[0]\n",
    "        x = x.reshape(-1, self.hidden_dim)  # flatten for masking and y_hat\n",
    "        x = x[mask]\n",
    "        x = F.gelu(x)\n",
    "        x = self.linear1(x)\n",
    "        y_hat = F.sigmoid(x)\n",
    "        y_hat = y_hat.view(-1)\n",
    "        if y is not None:\n",
    "            loss = torch.nn.functional.binary_cross_entropy(y_hat, y.float())\n",
    "            return y_hat, loss\n",
    "        return y_hat\n",
    "\n",
    "    def x_mask(self, x):\n",
    "        \"\"\"returns mask of shape (batch_size, seq_len)\"\"\"\n",
    "        mask = torch.sum(x, dim=2) != 0\n",
    "        mask[:, 0] = False\n",
    "        mask = mask.view(-1)\n",
    "        return mask\n",
    "\n",
    "    def predict(self, x):\n",
    "        y_hat = self.forward(x)\n",
    "        y_hat = (y_hat > 0.5).int()\n",
    "        return y_hat"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_curve(metrics):\n",
    "    plt.style.use('dark_background')\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
    "    axes[0].plot(metrics['train_loss'], label='train')\n",
    "    axes[0].plot(metrics['valid_loss'], label='val')\n",
    "    axes[0].set_title('Loss')\n",
    "    axes[0].legend()\n",
    "    axes[1].plot(metrics['train_f1'], label='train')\n",
    "    axes[1].plot(metrics['valid_f1'], label='val')\n",
    "    axes[1].set_title('F1')\n",
    "    axes[1].legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(metrics, model, train_batches, valid_batches, steps=2):\n",
    "    for batch_name, batches in [('train', train_batches), ('valid', valid_batches)]:\n",
    "        loss, f1 = evaluate_split(model, batches, steps=steps)\n",
    "        metrics[batch_name + '_loss'].append(loss)\n",
    "        metrics[batch_name + '_f1'].append(f1)\n",
    "        model.eval()\n",
    "    model.train()\n",
    "    return metrics\n",
    "\n",
    "def evaluate_split(model, batches, steps):\n",
    "    f1_scores, losses = [], []\n",
    "    for i in range(steps):\n",
    "        x, y = next(batches)\n",
    "        y_hat, loss = model(x, y)\n",
    "        losses.append(loss.item())\n",
    "        y_hat = model.predict(x).cpu().numpy().astype(int)\n",
    "        y = y.cpu().numpy().astype(int)\n",
    "        f1_scores.append(f1_score(y, y_hat))\n",
    "    return np.mean(losses), np.mean(f1_scores)\n",
    "\n",
    "def train(model, optimizer, train_batches, valid_batches, n_steps):\n",
    "    metrics = {'train_loss': [], 'train_f1': [], 'valid_loss': [], 'valid_f1': []}\n",
    "    for i in tqdm(range(n_steps)):\n",
    "        x, y = next(train_batches)\n",
    "        y_hat, loss = model(x, y)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if i % (n_steps // 100) == 0:\n",
    "            metrics = evaluate(metrics, model, train_batches, valid_batches)\n",
    "    final = evaluate(metrics, model, train_batches, valid_batches, steps=100)\n",
    "    return metrics, {k: v[-1] for k, v in final.items()}\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = SiameseNet({'hidden_dim': 128, 'dropout': 0.1}, embed_dim=384).to(device)\n",
    "train_batches = get_pair_batches(dataset_2['train'], device)\n",
    "valid_batches = get_pair_batches(dataset_2['valid'], device)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-3)\n",
    "metrics, _ = train(model, optimizer, train_batches, valid_batches, n_steps=1000)\n",
    "training_curve(metrics)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hyper_params():\n",
    "    \"\"\"return random hyperparameters\"\"\"\n",
    "    return {\n",
    "        'lr': 10 ** random.uniform(-5, -2),\n",
    "        'dropout': random.uniform(0, 0.5),\n",
    "        'hidden_dim': random.randint(64, 256),\n",
    "        'batch_size': [16, 32, 64, 128][random.randint(0, 2)],\n",
    "        'n_steps': [1000, 2000, 4000][random.randint(0, 2)]\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hyper_param_search(model_fn, batch_fn, dataset, n_trials=10):\n",
    "    \"\"\"search hyperparameters for a given model and dataset\"\"\"\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    hyper_param_metrics = []\n",
    "    for i in tqdm(range(n_trials)):\n",
    "        config = hyper_params()\n",
    "        model = model_fn(config).to(device)\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=config['lr'])\n",
    "        train_batches = batch_fn(dataset['train'], device, config['batch_size'])\n",
    "        valid_batches = batch_fn(dataset['valid'], device, config['batch_size'])\n",
    "        _, final= train(model, optimizer, train_batches, valid_batches, config['n_steps'])\n",
    "        hyper_param_metrics.append({**config, **final})\n",
    "    df = pd.DataFrame(hyper_param_metrics).sort_values('valid_f1', ascending=False)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(10):\n",
    "    x, y = next(batches)\n",
    "    y_hat, loss = model(x, y)\n",
    "    print(x.shape, y.shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_metrics(metrics):\n",
    "    # black theme matplotlib\n",
    "    # set font to serif\n",
    "    plt.rcParams['font.family'] = 'serif'\n",
    "    plt.style.use('dark_background')\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
    "    f1_valid = metrics['valid']['f1']\n",
    "    f1_train = metrics['train']['f1']\n",
    "    loss_valid = metrics['valid']['loss']\n",
    "    loss_train = metrics['train']['loss']\n",
    "    axes[0].plot(f1_valid, label='valid')\n",
    "    axes[0].plot(f1_train, label='train')\n",
    "    axes[0].set_title('f1')\n",
    "    axes[0].legend()\n",
    "    axes[1].plot(loss_valid, label='valid')\n",
    "    axes[1].plot(loss_train, label='train')\n",
    "    axes[1].set_title('loss')\n",
    "    axes[1].legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = SiameseNet(hidden_dim=42, dropout=0.3).to(device)\n",
    "batch_size = 16\n",
    "train_batches = get_pair_batches(paired_samples(data['2']['train']), batch_size, device)\n",
    "valid_batches = get_pair_batches(paired_samples(data['2']['valid']), batch_size, device)\n",
    "metrics = train(model, train_batches, valid_batches, 6000)\n",
    "plot_metrics(metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
