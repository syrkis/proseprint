{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi Author Writing Style Analysis\n",
    "by: Noah Syrkis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn, optim\n",
    "import wandb\n",
    "import os\n",
    "import json\n",
    "from collections import Counter, defaultdict\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import warnings; warnings.simplefilter('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = 'data/pan22/'\n",
    "PAD = '<PAD>'  # id 0\n",
    "UNK = '<UNK>'  # id 1"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_files(dataset, split='train', data_path=data_path):\n",
    "    folder_path = os.path.join(data_path, dataset, split)\n",
    "    files = os.listdir(folder_path)\n",
    "    files = make_pairs(files, folder_path)\n",
    "    return files\n",
    "\n",
    "\n",
    "def make_pairs(files, folder_path):\n",
    "    # there are two files for each sample problem-id.txt and truth-problem-id.json\n",
    "    # we want to pair them up\n",
    "    pairs = []\n",
    "    for f in files:\n",
    "        if f.endswith('.txt'):\n",
    "            truth_file = 'truth-' + f.replace('.txt', '.json')\n",
    "            pairs.append((os.path.join(folder_path, f), os.path.join(folder_path, truth_file)))\n",
    "    return pairs\n",
    "\n",
    "\n",
    "def get_vocab(dataset, n_vocab=10000, split='train', data_path=data_path):\n",
    "    files = get_files(dataset, split, data_path)\n",
    "    freqs = Counter()\n",
    "    for f in files:\n",
    "        with open(f[0], 'r') as f:\n",
    "            for line in f:\n",
    "                freqs.update(line.strip().split())\n",
    "    vocab = dict(freqs.most_common(n_vocab)).keys()\n",
    "    vocab = [PAD, UNK] + list(vocab)\n",
    "    idx2word = defaultdict(lambda: '<unk>', {i: w for i, w in enumerate(vocab)})\n",
    "    word2idx = defaultdict(lambda: 1, {w: i for i, w in enumerate(vocab)})\n",
    "    return word2idx, idx2word\n",
    "\n",
    "\n",
    "def plot_seqs(seq1, seq2, title):\n",
    "    # plot loss and log loss\n",
    "    # black background\n",
    "    plt.style.use('dark_background')\n",
    "    _, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
    "    axes[0].plot(seq1, label='train', color='white')\n",
    "    axes[0].plot(seq2, label='val', color='grey')\n",
    "    axes[0].set_title('Loss', color='white')\n",
    "    axes[0].legend()\n",
    "    axes[1].plot(np.log(seq1), label='train', color='white')\n",
    "    axes[1].plot(np.log(seq2), label='val', color='grey')\n",
    "    axes[1].set_title('Log Loss', color='white')\n",
    "    axes[1].legend()\n",
    "    plt.suptitle(title)\n",
    "    plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define dataset class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, dataset, split='train', data_path=data_path, max_len=32, word2idx=None, idx2word=None):\n",
    "        self.files = get_files(dataset, split, data_path)\n",
    "        if word2idx is None or idx2word is None:\n",
    "            self.word2idx, self.idx2word = get_vocab(dataset, split=split)\n",
    "        else:\n",
    "            self.word2idx = word2idx\n",
    "            self.idx2word = idx2word\n",
    "        self.pad = torch.nn.utils.rnn.pad_sequence\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text_file, truth_file = self.files[idx]\n",
    "        with open(text_file, 'r') as f:\n",
    "            text = f.readlines()\n",
    "            text = self.preprocess(text)\n",
    "        with open(truth_file, 'r') as f:\n",
    "            truth = json.load(f)\n",
    "            authors = torch.tensor(truth['authors']).reshape(1)\n",
    "            changes = torch.tensor(truth['changes'])\n",
    "            paragraph_authors = torch.tensor(truth['paragraph-authors'] )\n",
    "        return text, authors.float(), changes.float(), paragraph_authors.float()\n",
    "\n",
    "    # method for opening and processing the data\n",
    "    def preprocess(self, text):\n",
    "        ids_seq_list = [list(map(lambda x: self.word2idx[x], line.strip().split())) for line in text]\n",
    "        # pad and truncate\n",
    "        ids_seq_list = [line[:self.max_len] for line in ids_seq_list]\n",
    "        # pad\n",
    "        ids_seq_list = [line + [0] * (self.max_len - len(line)) for line in ids_seq_list]\n",
    "        return torch.tensor(ids_seq_list)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define model class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(torch.nn.Module):\n",
    "    # samples are lists of lists of integers\n",
    "    # target for now is just author count (int)\n",
    "    # model first determines the number of authors\n",
    "    # then determines at which paragraphs the authors change\n",
    "    # then determines which authors are at which paragraphs\n",
    "    # the model does not use batches of samples, but rather a single sample\n",
    "    def __init__(self, params):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(params['vocab_size'], params['embedding_dim'])\n",
    "        self.lstm = nn.LSTM(params['embedding_dim'], params['hidden_dims'][0])\n",
    "        self.linear = nn.Linear(params['hidden_dims'][0], params['hidden_dims'][1])\n",
    "\n",
    "        self.a_lstm = nn.LSTM(params['hidden_dims'][1], params['hidden_dims'][2])\n",
    "        self.a_linear = nn.Linear(params['hidden_dims'][2], 1)\n",
    "\n",
    "        self.c_lstm = nn.LSTM(params['hidden_dims'][1], params['hidden_dims'][2])\n",
    "        self.c_linear = nn.Linear(params['hidden_dims'][2], 1)\n",
    "\n",
    "        self.pa_lstm = nn.LSTM(params['hidden_dims'][1], params['hidden_dims'][2])\n",
    "        self.pa_linear = nn.Linear(params['hidden_dims'][2], 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)\n",
    "        x, _ = self.lstm(x)\n",
    "        x = self.linear(x)\n",
    "        x = x[:, -1, :]\n",
    "        a = self.authors(x)\n",
    "        c = self.changes(x)\n",
    "        pa = self.paragraph_authors(x, int(a))\n",
    "        return a, c, pa\n",
    "\n",
    "    def authors(self, x):\n",
    "        x, _ = self.a_lstm(x)\n",
    "        x = self.a_linear(x)\n",
    "        return x[-1]\n",
    "    \n",
    "    def changes(self, x):\n",
    "        x, _ = self.c_lstm(x)\n",
    "        x = self.c_linear(x)\n",
    "        return x[1:].reshape(-1)\n",
    "    \n",
    "    def paragraph_authors(self, x, n_authors):\n",
    "        x, _ = self.pa_lstm(x)\n",
    "        x = self.pa_linear(x)\n",
    "        x = torch.softmax(x, dim=1)\n",
    "        return x"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, ds_train, ds_valid, criterion, optimizer, epochs=10):\n",
    "    wandb.watch(model)\n",
    "    for epoch in range(epochs):\n",
    "        for x, a, c, pa in (pbar := tqdm(ds_train)):\n",
    "            optimizer.zero_grad()\n",
    "            a_hat, c_hat, pa_hat = model(x)\n",
    "            a_loss = criterion(a, a_hat)\n",
    "            c_loss = criterion(c, c_hat)\n",
    "            pa_loss = criterion(pa, pa_hat)\n",
    "            loss = a_loss + c_loss + pa_loss\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            pbar.set_description(f'Epoch {epoch + 1}, Train Loss: {loss.item():.4f}')\n",
    "            wandb.log({'train_loss': loss.item()})\n",
    "        with torch.no_grad():\n",
    "            for x, a, c, pa in (pbar := tqdm(ds_valid)):\n",
    "                a_hat, c_hat, pa_hat = model(x)\n",
    "                a_loss = criterion(pa, a_hat)\n",
    "                c_loss = criterion(c, c_hat)\n",
    "                pa_loss = criterion(pa, pa_hat)\n",
    "                loss = a_loss + c_loss + pa_loss\n",
    "                pbar.set_description(f'Epoch {epoch + 1}, Valid Loss: {loss.item():.4f}')\n",
    "                wandb.log({'valid_loss': loss.item()})"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instantiate dataset, model, and train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_train = Dataset('dataset1', split='train')\n",
    "ds_valid = Dataset('dataset1', split='validation', word2idx=ds_train.word2idx, idx2word=ds_train.idx2word)\n",
    "params = {'embedding_dim': 50, 'vocab_size': len(ds_train.word2idx), 'epochs': 10, 'hidden_dims': [100, 50, 25]}\n",
    "model = Model(params)\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "wandb.init(project='mawsa', config=params)\n",
    "train(model, ds_train, ds_valid, criterion, optimizer, epochs=1)\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
