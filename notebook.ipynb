{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi Author Writing Style Analysis\n",
    "\n",
    "The following notebook presents three different approaches to the problem of distingushing when in a sequence of paragraphs,\n",
    "the author changes. The first approach disregrads the order of the paragraphs, opting instead to view samples as pairs of paragraphs.\n",
    "It processes the paragraphs with a siamese network, which is a neural network that takes two inputs and outputs a single value.\n",
    "The second approach adds a recurrent layer to the siamese network, allowing it to take into account a sequence of paragraphs.\n",
    "The third approach builds on the second by augmenting the input with a manually engineered feature vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import pickle\n",
    "import torch\n",
    "import warnings\n",
    "from functools import partial\n",
    "from torch.nn import functional as F\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from tqdm import tqdm\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.metrics import f1_score, accuracy_score\n",
    "import pandas as pd\n",
    "import nltk\n",
    "import yaml\n",
    "from src.utils import get_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "warnings.filterwarnings('ignore')\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data\n",
    "\n",
    "We load the data (uncomment to reconstruct the data from the raw files),\n",
    "and make two data batch loaders:\n",
    "\n",
    "1. pairs of paragraphs, which will be used for our baseline siamese network.\n",
    "2. Sequences of paragraphs, which will be used for our recurrent siamese network.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nltk.download('averaged_perceptron_tagger')\n",
    "# data = { str(i): get_data(i) for i in range(1, 4) }\n",
    "# pickle.dump(data, open('data/data.pkl', 'wb'))\n",
    "data = pickle.load(open('data/data.pkl', 'rb'))\n",
    "dataset_1, dataset_2, dataset_3 = data['1'], data['2'], data['3']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def paired_samples(data_split, syntactic_flag):\n",
    "    \"\"\"turns data set into pair of consectuve sentences (flattens multi paragraph samples into pairs)\"\"\"\n",
    "    pairs = []\n",
    "    for problem_id in data_split.keys():\n",
    "        semantic = data_split[problem_id]['semantic']\n",
    "        syntactic = data_split[problem_id]['syntactic']\n",
    "        # concatenate all symantic and syntactic features into one vecntor per sample\n",
    "        if syntactic_flag:\n",
    "            texts = [np.concatenate([semantic[i], syntactic[i]]) for i in range(len(semantic))]\n",
    "        else:\n",
    "            texts = data_split[problem_id]['semantic']\n",
    "        targets = data_split[problem_id]['truth']['changes']\n",
    "        if len(texts) - 1 != len(targets):\n",
    "            # TODO: fix. a few of the samples have more than one paragraph, making .readlines() wrong\n",
    "            # print(f'problem {problem_id} has {len(texts)} texts and {len(targets)} targets')\n",
    "            continue\n",
    "        for target, text1, text2 in zip(targets, texts[:-1], texts[1:]):\n",
    "            pairs.append((text1, text2, target))\n",
    "    random.shuffle(pairs)\n",
    "    return pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pair_batches(data_split, device, syntactic=True, batch_size=32):\n",
    "    pairs = paired_samples(data_split, syntactic)\n",
    "    while True:\n",
    "        # perm = np.random.permutation(len(pairs))\n",
    "        x1 = torch.tensor(np.array([p[0] for p in pairs])).float().to(device)\n",
    "        x2 = torch.tensor(np.array([p[1] for p in pairs])).float().to(device)\n",
    "        y = torch.tensor(np.array([p[2] for p in pairs])).float().to(device)\n",
    "        perm = torch.randperm(len(pairs))\n",
    "        for i in range(0, len(pairs), batch_size):\n",
    "            batch = perm[i:i+batch_size]\n",
    "            yield (x1[batch], x2[batch]), y[batch]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sequence_batches(data_split, device, syntactic_flag=True, batch_size=32):\n",
    "    \"\"\"turns data set into sequence of sentences (flattens multi paragraph samples into sequence)\"\"\"\n",
    "    x, y = [], []\n",
    "    for problem_id in data_split.keys():\n",
    "        semantic = data_split[problem_id]['semantic']\n",
    "        syntactic = data_split[problem_id]['syntactic']\n",
    "        # concatenate all symantic and syntactic features into one vecntor per sample\n",
    "        if syntactic_flag:\n",
    "            texts = [np.concatenate([semantic[i], syntactic[i]]) for i in range(len(semantic))]\n",
    "        else:\n",
    "            texts = data_split[problem_id]['semantic']\n",
    "        targets = data_split[problem_id]['truth']['changes']\n",
    "        if len(texts) - 1 != len(targets):\n",
    "            continue\n",
    "        x.append(torch.tensor(texts))\n",
    "        y.append(torch.tensor(targets))\n",
    "    while True:\n",
    "        perm = torch.randperm(len(x))\n",
    "        for i in range(0, len(x), batch_size):\n",
    "            batch = perm[i:i+batch_size]\n",
    "            x_batch = [x[i] for i in batch]\n",
    "            y_batch = [y[i] for i in batch]\n",
    "            y_batch = torch.cat(y_batch, dim=0).to(device)\n",
    "            # pad with zero vectors\n",
    "            x_batch = pad_sequence(x_batch, batch_first=True, padding_value=0).to(device)\n",
    "            x_batch = x_batch.float()\n",
    "            yield x_batch, y_batch"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Models\n",
    "\n",
    "We make our two models, the siamese network and the recurrent siamese network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SiameseNet(torch.nn.Module):\n",
    "    def __init__(self, config, embed_dim):\n",
    "        super().__init__()\n",
    "        self.embed_dim = embed_dim\n",
    "        self.hidden_dim = config['hidden_dim']\n",
    "        self.linear1 = torch.nn.Linear(self.embed_dim, self.hidden_dim)\n",
    "        self.linear2 = torch.nn.Linear(self.hidden_dim, self.hidden_dim)\n",
    "        self.linear3 = torch.nn.Linear(self.hidden_dim, 1)\n",
    "        self.dropout = torch.nn.Dropout(config['dropout'])\n",
    "        self.sigmoid = torch.nn.Sigmoid()\n",
    "    \n",
    "    def forward(self, x, y=None):\n",
    "        x1, x2 = x\n",
    "        x1_hat = self.dropout(x1)\n",
    "        x1_hat = self.linear1(x1_hat)\n",
    "        x1_hat = F.gelu(x1_hat)\n",
    "        x2_hat = self.dropout(x2)\n",
    "        x2_hat = self.linear1(x2_hat)\n",
    "        x2_hat = F.gelu(x2_hat)\n",
    "        y_hat = torch.abs(x1_hat - x2_hat)\n",
    "        y_hat = self.linear2(y_hat)\n",
    "        y_hat = F.gelu(y_hat)\n",
    "        y_hat = self.dropout(y_hat)\n",
    "        y_hat = self.linear3(y_hat)\n",
    "        y_hat = self.sigmoid(y_hat)\n",
    "        if y is not None:\n",
    "            loss = torch.nn.functional.binary_cross_entropy(y_hat, y.float().unsqueeze(1))\n",
    "            return y_hat, loss\n",
    "        return y_hat\n",
    "\n",
    "    def predict(self, x):\n",
    "        y_hat = self.forward(x)\n",
    "        y_hat = y_hat.squeeze(1)\n",
    "        y_hat = (y_hat > 0.5).int()\n",
    "        return y_hat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RecurrentSiameseNet(torch.nn.Module):\n",
    "    def __init__(self, config, embed_dim):\n",
    "        super().__init__()\n",
    "        self.embed_dim = embed_dim\n",
    "        self.hidden_dim = config['hidden_dim']\n",
    "        self.linear1 = torch.nn.Linear(self.embed_dim, self.hidden_dim)\n",
    "        self.gru = torch.nn.GRU(self.hidden_dim, self.hidden_dim, batch_first=True)\n",
    "        self.linear2 = torch.nn.Linear(self.hidden_dim, 1)\n",
    "        self.dropout = torch.nn.Dropout(config['dropout'])\n",
    "    \n",
    "    def forward(self, x, y=None):\n",
    "        # x is tensor of shape (batch_size, seq_len, embed_dim)\n",
    "        mask = self.x_mask(x)\n",
    "        y_hat = self.dropout(x)\n",
    "        y_hat = y_hat.reshape(-1, self.embed_dim)\n",
    "        y_hat = self.linear1(y_hat) \n",
    "        y_hat = F.gelu(y_hat)\n",
    "        y_hat = y_hat.reshape(-1, x.shape[1], self.hidden_dim)\n",
    "        y_hat = self.gru(y_hat)[0]\n",
    "        y_hat = y_hat.reshape(-1, self.hidden_dim)  # flatten for masking and y_hat\n",
    "        y_hat = y_hat[mask]\n",
    "        y_hat = F.gelu(y_hat)\n",
    "        y_hat = self.linear2(y_hat)\n",
    "        y_hat = F.sigmoid(y_hat)\n",
    "        y_hat = y_hat.view(-1)\n",
    "        if y is not None:\n",
    "            try: \n",
    "                loss = torch.nn.functional.binary_cross_entropy(y_hat, y.float())\n",
    "            except ValueError:\n",
    "                return y_hat, None \n",
    "            return y_hat, loss\n",
    "        return y_hat\n",
    "\n",
    "    def x_mask(self, x):\n",
    "        \"\"\"returns mask of shape (batch_size, seq_len)\"\"\"\n",
    "        mask = torch.sum(x, dim=2) != 0\n",
    "        mask[:, 0] = False\n",
    "        mask = mask.view(-1)\n",
    "        return mask\n",
    "\n",
    "    def predict(self, x):\n",
    "        y_hat = self.forward(x)\n",
    "        y_hat = (y_hat > 0.5).int()\n",
    "        return y_hat"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training\n",
    "\n",
    "We define our training and evaluation functions, for use by both models, and all three datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_curve(metrics):\n",
    "    plt.style.use('dark_background')\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(16, 4))\n",
    "    axes[0].plot(metrics['train_loss'], label='train')\n",
    "    axes[0].plot(metrics['valid_loss'], label='val')\n",
    "    axes[0].set_title('Loss')\n",
    "    axes[0].legend()\n",
    "    axes[1].plot(metrics['train_f1'], label='train')\n",
    "    axes[1].plot(metrics['valid_f1'], label='val')\n",
    "    axes[1].set_title('F1')\n",
    "    axes[1].legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(metrics, model, train_batches, valid_batches, steps=10):\n",
    "    for batch_name, batches in [('train', train_batches), ('valid', valid_batches)]:\n",
    "        loss, f1, acc = evaluate_split(model, batches, steps=steps)\n",
    "        metrics[batch_name + '_loss'].append(loss)\n",
    "        metrics[batch_name + '_f1'].append(f1)\n",
    "        metrics[batch_name + '_acc'].append(acc)\n",
    "        model.eval()\n",
    "    model.train()\n",
    "    return metrics\n",
    "\n",
    "def evaluate_split(model, batches, steps):\n",
    "    f1_scores, losses, acc_scores = [], [], []\n",
    "    for i in range(steps):\n",
    "        x, y = next(batches)\n",
    "        y_hat, loss = model(x, y)\n",
    "        if loss is None:\n",
    "            continue\n",
    "        losses.append(loss.item())\n",
    "        y_hat = model.predict(x).cpu().numpy().astype(int)\n",
    "        y = y.cpu().numpy().astype(int)\n",
    "        f1_scores.append(f1_score(y, y_hat))\n",
    "        acc_scores.append(accuracy_score(y, y_hat))\n",
    "    return np.mean(losses), np.mean(f1_scores), np.mean(acc_scores)\n",
    "\n",
    "def train(model, optimizer, train_batches, valid_batches=None, batch_size=32, n_steps=1000):\n",
    "    # returns metrics and final scores, if doing validation, else returns final model for testing\n",
    "    metrics = {'train_loss': [], 'train_f1': [], 'train_acc': [], 'valid_loss': [], 'valid_f1': [], 'valid_acc': []}\n",
    "    for i in range(n_steps):\n",
    "        x, y = next(train_batches)\n",
    "        y_hat, loss = model(x, y)\n",
    "        if loss is None:  # there is an extremly rare bug where y_hat is one short of y FIXME.\n",
    "            continue\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if valid_batches and i % (n_steps // 100) == 0:\n",
    "            metrics = evaluate(metrics, model, train_batches, valid_batches)\n",
    "    if valid_batches:\n",
    "        final = evaluate(metrics, model, train_batches, valid_batches, steps=4200 // batch_size)\n",
    "        return metrics, {k: v[-1] for k, v in final.items()}\n",
    "    return model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment\n",
    "\n",
    "To do hyper paramter tuning, and test performance of our models on our three datasets, we define the experiment functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hyper_params():\n",
    "    \"\"\"return random hyperparameters\"\"\"\n",
    "    return {\n",
    "        'lr': 10 ** random.choice([-3, -4, -5]),\n",
    "        'dropout': random.choice([0.1, 0.2, 0.3]),\n",
    "        'hidden_dim': random.choice([64, 128, 256]),\n",
    "        'batch_size': random.choice([16, 32, 64]),\n",
    "        'n_steps': random.choice([1000, 2000, 4000, 6000])\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hyper_param_search(model_fn, batch_fn, dataset, syntac_bool, n_trials=10):\n",
    "    \"\"\"search hyperparameters for a given model and dataset\"\"\"\n",
    "    embed_dim = 384 + 61 if syntac_bool else 384\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    hyper_param_metrics = []\n",
    "    training_metrics_list = []\n",
    "    for i in tqdm(range(n_trials)):\n",
    "        config = hyper_params()\n",
    "        model = model_fn(config, embed_dim=embed_dim).to(device)\n",
    "        optimizer = torch.optim.AdamW(model.parameters(), lr=config['lr'])\n",
    "        train_batches = batch_fn(dataset['train'], device, batch_size=config['batch_size'])\n",
    "        valid_batches = batch_fn(dataset['valid'], device, batch_size=config['batch_size'])\n",
    "        training_metrics, final= train(model, optimizer, train_batches, valid_batches, config['batch_size'], config['n_steps'])\n",
    "        training_metrics_list.append(training_metrics)\n",
    "        hyper_param_metrics.append({**config, **final})\n",
    "    df = pd.DataFrame(hyper_param_metrics).sort_values('valid_f1', ascending=False)\n",
    "    return df, training_metrics_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def experiment(n_trials=10):\n",
    "    for ds, ds_name in [(dataset_1, 'dataset_1'), (dataset_2, 'dataset_2'), (dataset_3, 'dataset_3')]:\n",
    "        for syntax_bool in [True, False]:\n",
    "            get_sequence_batches_fn = partial(get_sequence_batches, syntactic_flag=syntax_bool)\n",
    "            get_pair_batches_fn = partial(get_pair_batches, syntactic=syntax_bool)\n",
    "            runs = [(RecurrentSiameseNet, get_sequence_batches_fn, ds), (SiameseNet, get_pair_batches_fn, ds)]\n",
    "            for model, batch_fn, ds in runs:\n",
    "                print(f'running {model.__name__}' + f\" on {ds_name} \" + f\"{'with' if syntax_bool else 'without'} syntax\")\n",
    "                df, training_metrics_list = hyper_param_search(model, batch_fn, ds, syntax_bool, n_trials=n_trials)\n",
    "                df_file_name = f'results/{model.__name__}_{ds_name}_{\"syntax\" if syntax_bool else \"no_syntax\"}.csv'\n",
    "                # round floats to 4 decimal places\n",
    "                df = df.round(4)\n",
    "                df.to_csv(df_file_name, index=False)\n",
    "                training_metrics_file_name = f'results/{model.__name__}_{ds_name}_{\"syntax\" if syntax_bool else \"no_syntax\"}_training_metrics.pkl'\n",
    "                pickle.dump(training_metrics_list, open(training_metrics_file_name, 'wb'))\n",
    "                print(f'saved {df_file_name} and {training_metrics_file_name}')\n",
    "                print(f'best hyperparameters: {df.iloc[0]}')\n",
    "                print(f'best f1 score: {df.iloc[0][\"valid_f1\"]}')\n",
    "                print(f'best accuracy score: {df.iloc[0][\"valid_acc\"]}')\n",
    "                print()\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running RecurrentSiameseNet on dataset_1 with syntax\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [06:04<00:00, 18.22s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved results/RecurrentSiameseNet_dataset_1_syntax.csv and results/RecurrentSiameseNet_dataset_1_syntax_training_metrics.pkl\n",
      "best hyperparameters: lr               0.0010\n",
      "dropout          0.2000\n",
      "hidden_dim      64.0000\n",
      "batch_size      16.0000\n",
      "n_steps       6000.0000\n",
      "train_loss       0.1586\n",
      "train_f1         0.9634\n",
      "train_acc        0.9349\n",
      "valid_loss       0.1942\n",
      "valid_f1         0.9558\n",
      "valid_acc        0.9227\n",
      "Name: 9, dtype: float64\n",
      "best f1 score: 0.9558\n",
      "best accuracy score: 0.9227\n",
      "\n",
      "running SiameseNet on dataset_1 with syntax\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [03:27<00:00, 10.38s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved results/SiameseNet_dataset_1_syntax.csv and results/SiameseNet_dataset_1_syntax_training_metrics.pkl\n",
      "best hyperparameters: lr               0.0010\n",
      "dropout          0.1000\n",
      "hidden_dim      64.0000\n",
      "batch_size      64.0000\n",
      "n_steps       4000.0000\n",
      "train_loss       0.1601\n",
      "train_f1         0.9629\n",
      "train_acc        0.9338\n",
      "valid_loss       0.2036\n",
      "valid_f1         0.9558\n",
      "valid_acc        0.9220\n",
      "Name: 13, dtype: float64\n",
      "best f1 score: 0.9558\n",
      "best accuracy score: 0.922\n",
      "\n",
      "running RecurrentSiameseNet on dataset_1 without syntax\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [05:38<00:00, 16.90s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved results/RecurrentSiameseNet_dataset_1_no_syntax.csv and results/RecurrentSiameseNet_dataset_1_no_syntax_training_metrics.pkl\n",
      "best hyperparameters: lr               0.0010\n",
      "dropout          0.3000\n",
      "hidden_dim     128.0000\n",
      "batch_size      32.0000\n",
      "n_steps       1000.0000\n",
      "train_loss       0.2754\n",
      "train_f1         0.9409\n",
      "train_acc        0.8924\n",
      "valid_loss       0.2897\n",
      "valid_f1         0.9363\n",
      "valid_acc        0.8851\n",
      "Name: 7, dtype: float64\n",
      "best f1 score: 0.9363\n",
      "best accuracy score: 0.8851\n",
      "\n",
      "running SiameseNet on dataset_1 without syntax\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [02:56<00:00,  8.83s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved results/SiameseNet_dataset_1_no_syntax.csv and results/SiameseNet_dataset_1_no_syntax_training_metrics.pkl\n",
      "best hyperparameters: lr               0.0001\n",
      "dropout          0.1000\n",
      "hidden_dim     256.0000\n",
      "batch_size      64.0000\n",
      "n_steps       4000.0000\n",
      "train_loss       0.1277\n",
      "train_f1         0.9737\n",
      "train_acc        0.9531\n",
      "valid_loss       0.2242\n",
      "valid_f1         0.9447\n",
      "valid_acc        0.9035\n",
      "Name: 7, dtype: float64\n",
      "best f1 score: 0.9447\n",
      "best accuracy score: 0.9035\n",
      "\n",
      "running RecurrentSiameseNet on dataset_2 with syntax\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 65%|██████▌   | 13/20 [05:28<02:41, 23.05s/it]"
     ]
    }
   ],
   "source": [
    "# experiment(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dfs():\n",
    "    dfs = {}\n",
    "    for ds_name in ['dataset_1', 'dataset_2', 'dataset_3']:\n",
    "        for model in [RecurrentSiameseNet, SiameseNet]:\n",
    "            file_name = f'results/{model.__name__}_{ds_name}.csv'\n",
    "            df = pd.read_csv(file_name)\n",
    "            dfs[(model.__name__, ds_name)] = df\n",
    "    return dfs"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_splits(dataset):\n",
    "    \"\"\"merge train and valid splits\"\"\"\n",
    "    merged = {}\n",
    "    for split in ['train', 'valid']:\n",
    "        for problem_id in dataset[split].keys():\n",
    "            merged[problem_id] = dataset[split][problem_id]\n",
    "    return merged\n",
    "\n",
    "def test_seq():\n",
    "    with open('config.yaml', 'r') as f:\n",
    "        configs = yaml.load(f, Loader=yaml.FullLoader)\n",
    "    results = []\n",
    "    for dataset, ds_name in [(dataset_1, 'dataset_1'), (dataset_2, 'dataset_2'), (dataset_3, 'dataset_3')]:\n",
    "        for syntactic_flag in [True, False]:\n",
    "            for model_fn, batch_fn in [(RecurrentSiameseNet, get_sequence_batches), (SiameseNet, get_pair_batches)]:\n",
    "                config = configs[ds_name]['syntax' if syntactic_flag else 'no_syntax']['siamese' if model_fn == SiameseNet else 'recurrent']\n",
    "                train_data = merge_splits([dataset['train'], dataset['valid']])\n",
    "                test_data = dataset['test']\n",
    "                embed_dim = 384 + 61 if syntactic_flag else 384\n",
    "                device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "                model = model_fn({'hidden_dim': config['hidden_dim'], 'dropout': config['dropout']}, embed_dim=embed_dim).to(device)\n",
    "                optimizer = torch.optim.AdamW(model.parameters(), lr=config['lr'])\n",
    "                train_batches = batch_fn(train_data, device, batch_size=config['batch_size'])\n",
    "                test_batches = batch_fn(test_data, device, batch_size=config['batch_size'])\n",
    "                train(model, optimizer, train_batches, batch_size=config['batch_size'], n_steps=config['n_steps'])\n",
    "                loss, f1, acc = evaluate_split(model, test_batches, steps=len(test_data) // config['batch_size'])\n",
    "                print(f'{model_fn.__name__} on {dataset[\"name\"]} with {\"syntax\" if syntactic_flag else \"no syntax\"}: f1: {f1}, acc: {acc}, loss: {loss}')\n",
    "                result = {'model': model_fn.__name__, 'dataset': dataset['name'], 'syntax': syntactic_flag, 'f1': f1, 'acc': acc, 'loss': loss}\n",
    "                results.append(result)\n",
    "    df = pd.DataFrame(results)\n",
    "    df.to_csv('results/test_results.csv', index=False)\n",
    "    return df\n",
    "\n",
    "test_seq()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
